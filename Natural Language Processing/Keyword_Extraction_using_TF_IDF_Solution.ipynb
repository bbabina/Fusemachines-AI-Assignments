{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "bUy7MKoMrrCe",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "6affc2614f248681a0c06f302e18279a",
          "grade": false,
          "grade_id": "cell-4b44617a55e49104",
          "locked": true,
          "schema_version": 3,
          "solution": false
        }
      },
      "source": [
        "# Keywords Extraction Using TF-IDF\n",
        "<b><div style=\"text-align: right\">[TOTAL POINTS: 14]</div></b>\n",
        "\n",
        "In the reading material, you learned about the complete pipeline for an NLP task, including raw text extraction and processing, text preprocessing, and applying machine learning models. In this assignment, you will apply the knowledge you have learned to extract keywords from text articles. Here, you will scrape a text document, preprocess it, and extract keywords from it. Note that you won't be fitting any machine learning model in this assignment. Instead, you will use the Tfidf vectorization technique to extract the keywords.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "Qdt3R6OyODDA",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "19c6a3a32b1e6afb1f3f22a1ede77772",
          "grade": false,
          "grade_id": "cell-d33f06e294c51115",
          "locked": true,
          "schema_version": 3,
          "solution": false
        }
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTV8_7x7rHcf"
      },
      "outputs": [],
      "source": [
        "!pip install -q nltk\n",
        "!pip install -q scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "editable": false,
        "id": "Fj9Fojd4mmM8",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "1a5e04ba90b479e1b126c86e0a4aec73",
          "grade": false,
          "grade_id": "cell-ad76e3253f795b6c",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "outputId": "75bcfb67-f4c5-4692-9052-d48126e28255"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "wRslRDGyJmsa",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "4dca30fec8f91f84dc2ea8c84dcd9a2a",
          "grade": false,
          "grade_id": "cell-aa4a833c1314e53f",
          "locked": true,
          "schema_version": 3,
          "solution": false
        }
      },
      "source": [
        "## Dataset Description\n",
        "\n",
        "You will first fit the TF-IDF vectorizer on the [Medium Articles](https://www.kaggle.com/hsankesara/medium-articles) dataset. This dataset contains a collection of medium articles on ML, AI, and data science and other details like author, number of claps, reading time, etc.\n",
        "\n",
        "*Source:* https://www.kaggle.com/hsankesara/medium-articles\n",
        "\n",
        "*Author:* Heet Sankesara\n",
        "\n",
        "*License:* https://creativecommons.org/publicdomain/zero/1.0/\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "deletable": false,
        "editable": false,
        "id": "qnhdY-JNUKG2",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "cf0fbb89b2b02f898ad3d7283b023cab",
          "grade": false,
          "grade_id": "cell-ebb015804db302cf",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "outputId": "4dda0da2-38a2-4e7f-afeb-f86934032bf9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             author claps  reading_time  \\\n",
              "0        Justin Lee  8.3K            11   \n",
              "1       Conor Dewey  1.4K             7   \n",
              "2  William Koehrsen  2.8K            11   \n",
              "3      Gant Laborde  1.3K             7   \n",
              "4  Emmanuel Ameisen   935            11   \n",
              "\n",
              "                                                link  \\\n",
              "0  https://medium.com/swlh/chatbots-were-the-next...   \n",
              "1  https://towardsdatascience.com/python-for-data...   \n",
              "2  https://towardsdatascience.com/automated-featu...   \n",
              "3  https://medium.freecodecamp.org/machine-learni...   \n",
              "4  https://blog.insightdatascience.com/reinforcem...   \n",
              "\n",
              "                                               title  \\\n",
              "0  Chatbots were the next big thing: what happene...   \n",
              "1  Python for Data Science: 8 Concepts You May Ha...   \n",
              "2  Automated Feature Engineering in Python – Towa...   \n",
              "3  Machine Learning: how to go from Zero to Hero ...   \n",
              "4  Reinforcement Learning from scratch – Insight ...   \n",
              "\n",
              "                                                text  \n",
              "0  Oh, how the headlines blared:\\nChatbots were T...  \n",
              "1  If you’ve ever found yourself looking up the s...  \n",
              "2  Machine learning is increasingly moving from h...  \n",
              "3  If your understanding of A.I. and Machine Lear...  \n",
              "4  Want to learn about applied Artificial Intelli...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-76321732-47a6-42a7-9212-3d0b817af83b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author</th>\n",
              "      <th>claps</th>\n",
              "      <th>reading_time</th>\n",
              "      <th>link</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Justin Lee</td>\n",
              "      <td>8.3K</td>\n",
              "      <td>11</td>\n",
              "      <td>https://medium.com/swlh/chatbots-were-the-next...</td>\n",
              "      <td>Chatbots were the next big thing: what happene...</td>\n",
              "      <td>Oh, how the headlines blared:\\nChatbots were T...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Conor Dewey</td>\n",
              "      <td>1.4K</td>\n",
              "      <td>7</td>\n",
              "      <td>https://towardsdatascience.com/python-for-data...</td>\n",
              "      <td>Python for Data Science: 8 Concepts You May Ha...</td>\n",
              "      <td>If you’ve ever found yourself looking up the s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>William Koehrsen</td>\n",
              "      <td>2.8K</td>\n",
              "      <td>11</td>\n",
              "      <td>https://towardsdatascience.com/automated-featu...</td>\n",
              "      <td>Automated Feature Engineering in Python – Towa...</td>\n",
              "      <td>Machine learning is increasingly moving from h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Gant Laborde</td>\n",
              "      <td>1.3K</td>\n",
              "      <td>7</td>\n",
              "      <td>https://medium.freecodecamp.org/machine-learni...</td>\n",
              "      <td>Machine Learning: how to go from Zero to Hero ...</td>\n",
              "      <td>If your understanding of A.I. and Machine Lear...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Emmanuel Ameisen</td>\n",
              "      <td>935</td>\n",
              "      <td>11</td>\n",
              "      <td>https://blog.insightdatascience.com/reinforcem...</td>\n",
              "      <td>Reinforcement Learning from scratch – Insight ...</td>\n",
              "      <td>Want to learn about applied Artificial Intelli...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-76321732-47a6-42a7-9212-3d0b817af83b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-76321732-47a6-42a7-9212-3d0b817af83b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-76321732-47a6-42a7-9212-3d0b817af83b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b7f61229-2385-47b5-bf98-d8110c0b4c57\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b7f61229-2385-47b5-bf98-d8110c0b4c57')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b7f61229-2385-47b5-bf98-d8110c0b4c57 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "dataset = pd.read_csv('articles.csv')\n",
        "dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "editable": false,
        "id": "b_ZKbbxbFQNN",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "294c4eb1c25b46c6c11fe9ae15433963",
          "grade": false,
          "grade_id": "cell-16f93f2f9727080d",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "outputId": "c2418326-b354-490e-c18d-ebd006969282"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No. of instances: 337\n"
          ]
        }
      ],
      "source": [
        "print(\"No. of instances: {}\".format(dataset.shape[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "gKfejysw9xa-",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "ba30770bde21763f4dfff9643c532347",
          "grade": false,
          "grade_id": "cell-6d7cc01375dba29d",
          "locked": true,
          "schema_version": 3,
          "solution": false
        }
      },
      "source": [
        "For this assignment, you will use only the `text` column from the dataset. You won't have to preprocess the texts yourself, as you have already done it a couple of times. So let's quickly load the preprocessed articles from the file `preprocessed_articles.txt`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "GNyoct-eWkAP",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "9f5bac10a3b661796e83427e453a7f79",
          "grade": false,
          "grade_id": "cell-48c858f7136ed30c",
          "locked": true,
          "schema_version": 3,
          "solution": false
        }
      },
      "outputs": [],
      "source": [
        "# define an empty list to contain the preprocessed articles\n",
        "preprocessed_articles = []\n",
        "\n",
        "# open file and read its content\n",
        "with open('preprocessed_articles.txt', 'r') as filehandle:\n",
        "    for line in filehandle:\n",
        "        # remove linebreak which is the last character of the string\n",
        "        currentPlace = line[:-1]\n",
        "\n",
        "        # add item to the list\n",
        "        preprocessed_articles.append(currentPlace)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "editable": false,
        "id": "rk8JMJ8KbRzk",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f7e44cf576e307c40ed3dbc7529d4c14",
          "grade": false,
          "grade_id": "cell-887e028f9090b3e5",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "outputId": "9a4f6edb-560d-4530-f4dc-d0aa118ae173"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "oh headlin blare chatbot next big thing hope sky high bright eye bushi tail industri wa ripe new era innov wa time start social machin whi road sign point toward insan success mobil world congress chatbot main headlin confer organ cite overwhelm accept event inevit shift focu brand corpor chatbot fact onli signific question around chatbot wa would monopol field whether chatbot would take first place one year answer question becaus even ecosystem platform domin chatbot first technolog develop talk grandios term slump spectacularli age old hype cycl unfold familiar fashion expect built built kind fizzl predict paradim shift materi app tellingli still aliv well look back breathless optim turn slightli baffl wa chatbot revolut promis digit ethan bloch sum gener consensu accord dave feldman vice presid product design heap chatbot take one difficult problem fail took sever fail bot interfac user differ way big divid text vs speech begin comput interfac wa written word user type command manual machin get anyth done graphic user interfac gui came along save day becam entranc window mous click icon hey eventu got color meanwhil bunch research scientist busili develop natur languag nl interfac databas instead learn arcan databas queri languag anoth bunch scientist develop speech process softwar could speak comput rather type thi turn whole lot difficult anyon origin realis next item agenda wa hold two way dialog machin exampl dialog date back vcr setup system pretti cool right system take turn collabor way doe smart job figur user want wa care craft deal convers involv vcr could onli oper within strict limit modern day bot whether use type spoken input face challeng also work effici scalabl way varieti platform basic still tri achiev innov year ago think go wrong overs assumpt ha app would replac bot pit two dispar concept one anoth instead see separ entiti design serv differ purpos discourag bot develop might rememb similar war cri app first came onto scene ten year ago rememb app replac internet said new product servic need two follow better cheaper faster chatbot cheaper faster app yet least whether better subject think fair say today best bot compar today best app plu nobodi think use lyft complic hard order food buy dress app complic tri complet task bot bot fail great bot use averag app come rich sophist multi layer app competit becaus machin let us access vast complex inform system earli graphic inform system revolutionari leap forward help us locat system modern day app benefit decad research experiment whi would throw thi away swap word replac extend thing get much interest today success bot experi take hybrid approach incorpor chat broader strategi encompass tradit element next wave multimod app say want like siri get back inform map text even spoken respons anoth problemat aspect sweep natur hype tend bypass essenti question like plenti compani bot right solut past two year litter case bot blindli appli problem need build bot sake let loos hope best never end well vast major bot built use decis tree logic bot respons reli spot specif keyword user input advantag thi approach pretti easi list case design cover precis disadvantag becaus bot pure reflect capabl fastidi patienc person creat mani user need input abl anticip problem aris life refus fit box accord recent report bot facebook messeng fail fulfil simpl user request thi partli result develop fail narrow bot one strong area focu build growthbot decid make specif sale market rounder despit temptat get overexcit potenti capabilti rememb bot doe one thing well infinit help bot doe multipl thing poorli compet develop build basic bot minut one hold convers anoth stori despit constant hype around ai still long way achiev anyth remot human like ideal world technolog known nlp natur languag process allow chatbot understand messag receiv nlp onli emerg research lab veri much infanc platform provid bit nlp even best toddler level capac exampl think siri understand word mean matt asay outlin thi result anoth issu failur captur attent creativ develop convers complex linear topic spin around take random turn restart abruptli finish today rule base dialogu system brittl deal thi kind unpredict statist approach use machin learn limit level ai requir human like convers avail yet meantim high qualiti exampl trailblaz bot lead way dave feldman remark onc upon time onli way interact comput wa type arcan command termin visual interfac use window icon mous revolut manipul inform reason comput move text base graphic user interfac gui input side easier faster click type tap select obvious prefer type whole sentenc even predict often error prone text output side old adag pictur worth thousand word usual true love optic display inform becaus highli visual creatur accid kid love touch screen pioneer dreamt graphic interfac inspir cognit psycholog studi brain deal commun convers ui meant replic way human prefer commun end requir extra cognit effort essenti swap someth simpl complex altern sure concept onli express use languag show way get museum give step take longer minut task carri effici intuit gui convers ui aim human dimens busi interact make sens one thing broken sale market lack human brand hide behind ticket number feedback form repli email autom respons gate contact us form facebook goal bot pass call ture test mean tell whether talk bot human bot human never convers encompass much text human read line leverag contextu inform understand doubl layer like sarcasm bot quickli forget talk mean bit like convers someon ha littl short term memori hubspot team pinpoint peopl easili fool pretend bot human guarante diminish return mention fact lie user even rare bot power state art nlp excel process produc content fall short comparison thing convers ui built replic way human prefer commun human human prefer interact machin necessarili end day amount witti quip human like manner save bot convers failur way earli adopt entir wrong peopl yell googl home play favorit song order pizza domino bot get makeup tip sephora term consum respons develop involv chatbot live hype gener circa even close comput good comput search data crunch number analyz opinion condens inform comput good understand human emot state nlp mean still get ask never mind feel whi still imposs imagin effect custom support sale market without essenti human touch empathi emot intellig bot continu help us autom repetit low level task queri cog larger complex system ourselv disservic expect much soon whole stori ye industri massiv overestim initi impact chatbot would emphasi initi bill gate onc said hype good thing start examin middl ground grey area instead hyper inflat frantic black white zone believ veri begin explos growth thi sens anti climax complet normal transform technolog messag continu gain traction chatbot go away nlp ai becom sophist everi day develop app platform continu experi heavili invest convers market wait see happen next quick cheer stand ovat clap show much enjoy thi stori head growth growthbot messag convers strategi medium largest public maker subscrib receiv top stori\n",
            "ever found look question concept syntax program alon find thi constantli unnatur look thing stackoverflow resourc doe slow good bit rais question complet understand languag live world seemingli infinit amount access free resourc loom one search away time howev thi bless curs manag effect relianc resourc build poor habit set back long term person find pull code similar discuss thread sever time rather take time learn solidifi concept reproduc code next time thi approach lazi may path least resist short term ultim hurt growth product abil recal syntax cough interview line recent work onlin data scienc cours titl python data scienc machin learn udemi oh god sound like guy youtub earli lectur seri wa remind concept syntax consist overlook perform data analysi python interest solidifi understand concept onc save guy coupl stackoverflow search stuff alway forget work python numpi panda includ short descript exampl howev benefit also includ link video resourc explor concept depth well write loop everi time need defin sort list tediou luckili python ha built way address thi problem one line code syntax littl hard wrap head around onc get familiar thi techniqu use fairli often see exampl abov would normal go list comprehens loop vs creat list one simpl line loop necessari ever get tire creat function function limit use case lambda function rescu lambda function use creat small one time anonym function object python basic let creat function without creat function basic syntax lambda function note lambda function everyth regular function long one express check simpl exampl upcom video get better feel power lambda function onc grasp lambda function learn pair map filter function power tool specif map take list transform new list perform sort oper element thi exampl goe element map result time new list note list function simpli convert output list type filter function take list rule much like map howev return subset origin list compar element boolean filter rule creat quick easi numpi array look arang linspac function one ha specif purpos appeal instead use rang output numpi array typic easier work data scienc arang return evenli space valu within given interv along start stop point also defin step size data type necessari note stop point cut valu includ array output linspac veri similar slight twist linspac return evenli space number specifi interv given start stop point well number valu linspac evenli space numpi array thi especi help data visual declar axe plot may ran thi drop column panda sum valu numpi matrix sure point let use exampl drop column know mani time wrote thi line code befor actual knew whi wa declar axi wa probabl deduc abov set axi want deal column set want row whi thi favorit reason atleast rememb thi call shape attribut panda datafram give us back tupl first valu repres number row second valu repres number column think thi index python row column much like declar axi valu crazi right familiar sql concept probabl come lot easier anyhow function essenti way combin datafram specif way difficult keep track best use time let review concat allow user append one datafram either next depend defin axi merg combin multipl datafram specif common column serv primari key join much like merg combin two datafram howev join base indic rather specifi column check excel panda document specif syntax concret exampl well special case may run think appli map function made panda datafram specif seri familiar seri pretti similar numpi array part appli send function everi element along column row depend specifi might imagin use thi especi format manipul valu across whole datafram column without loop last certainli least pivot tabl familiar microsoft excel probabl heard pivot tabl respect panda built pivot tabl function creat spreadsheet style pivot tabl datafram note level pivot tabl store multiindex object index column result datafram hope coupl overview effect jog memori regard import yet somewhat tricki method function concept frequent encount use python data scienc person know even act write tri explain simpl term ha help ton interest receiv weekli rundown interest articl resourc focus data scienc machin learn artifici intellig subscrib self driven data scienc use form enjoy thi post feel free hit clap button interest post come make sure follow medium link write ship everi day thi month part day challeng thi articl wa origin publish conordewey com quick cheer stand ovat clap show much enjoy thi stori data scientist writer www conordewey com share concept idea code\n",
            "machin learn increasingli move hand design model automat optim pipelin use tool h tpot auto sklearn librari along method random search aim simplifi model select tune part machin learn find best model dataset littl manual intervent howev featur engin arguabl valuabl aspect machin learn pipelin remain almost entir human labor featur engin also known featur creation process construct new featur exist data train machin learn model thi step import actual model use becaus machin learn algorithm onli learn data give creat featur relev task absolut crucial see excel paper use thing know machin learn typic featur engin drawn manual process reli domain knowledg intuit data manipul thi process extrem tediou final featur limit human subject time autom featur engin aim help data scientist automat creat mani candid featur dataset best select use train thi articl walk exampl use autom featur engin featuretool python librari use exampl dataset show basic stay tune futur post use real world data complet code thi articl avail github featur engin mean build addit featur exist data often spread across multipl relat tabl featur engin requir extract relev inform data get singl tabl use train machin learn model process construct featur veri time consum becaus new featur usual requir sever step build especi use inform one tabl group oper featur creation two categori transform aggreg let look exampl see concept action transform act singl tabl think term python tabl panda datafram creat new featur one exist column exampl tabl client creat featur find month join column take natur log incom column transform becaus use inform onli one tabl hand aggreg perform across tabl use one mani relationship group observ calcul statist exampl anoth tabl inform loan client client may multipl loan calcul statist averag maximum minimum loan client thi process involv group loan tabl client calcul aggreg merg result data client data would python use languag panda oper difficult themselv hundr variabl spread across dozen tabl thi process feasibl hand ideal want solut automat perform transform aggreg across multipl tabl combin result data singl tabl although panda great resourc onli much data manipul want hand manual featur engin check excel python data scienc handbook fortun featuretool exactli solut look thi open sourc python librari automat creat mani featur set relat tabl featuretool base method known deep featur synthesi sound lot impos actual name come stack multipl featur becaus use deep learn deep featur synthesi stack multipl transform aggreg oper call featur primit vocab featuretool creat featur data spread across mani tabl like idea machin learn complex method built foundat simpl concept learn one build block time form good understand thi power method first let take look exampl data alreadi saw dataset abov complet collect tabl follow machin learn task predict whether client repay futur loan want combin inform client singl tabl tabl relat client id loan id variabl could use seri transform aggreg thi process hand howev shortli see instead use featuretool autom process first two concept featuretool entiti entityset entiti simpli tabl datafram think panda entityset collect tabl relationship think entityset anoth python data structur method attribut creat empti entityset featuretool use follow add entiti entiti must index column uniqu element valu index must appear tabl onli onc index client datafram client idbecaus client ha onli one row thi datafram add entiti exist index entityset use follow syntax loan datafram also ha uniqu index loan id syntax add thi entityset client howev payment datafram uniqu index add thi entiti entityset need pass paramet make index true specifi name index also although featuretool automat infer data type column entiti overrid thi pass dictionari column type paramet variabl type thi datafram even though miss integ thi numer variabl sinc onli take discret valu tell featuretool treat categor variabl ad datafram entityset inspect ani column type correctli infer modif specifi next need specifi tabl entityset relat best way think relationship two tabl analog parent child thi one mani relationship parent multipl children realm tabl parent tabl ha one row everi parent child tabl may multipl row correspond multipl children parent exampl dataset client datafram parent loan datafram client ha onli one row client may multipl row loan likewis loan parent payment becaus loan multipl payment parent link children share variabl perform aggreg group child tabl parent variabl calcul statist across children parent formal relationship featuretool onli need specifi variabl link two tabl togeth client loan tabl link via client id variabl loan payment link loan id syntax creat relationship ad entityset shown entityset contain three entiti tabl relationship link entiti togeth ad entiti formal relationship entityset complet readi make featur befor quit get deep featur synthesi need understand featur primit alreadi know call differ name simpli basic oper use form new featur new featur creat featuretool use primit either themselv stack multipl primit list featur primit featuretool also defin custom primit primit use themselv combin creat featur make featur specifi primit use ft df function stand deep featur synthesi pass entityset target entiti tabl want add featur select tran primit transform agg primit aggreg result datafram new featur client becaus made client target entiti exampl month client join transform featur primit also number aggreg primit averag payment amount client even though specifi onli featur primit featuretool creat mani new featur combin stack primit complet datafram ha column new featur piec place understand deep featur synthesi df fact alreadi perform df previou function call deep featur simpli featur made stack multipl primit df name process make featur depth deep featur number primit requir make featur exampl mean payment payment amount column deep featur depth becaus wa creat use singl aggreg featur depth two last loan mean payment payment amount thi made stack two aggreg last recent top mean thi repres averag payment size recent loan client stack featur ani depth want practic never gone beyond depth thi point featur difficult interpret encourag anyon interest tri go deeper manual specifi featur primit instead let featuretool automat choos featur us thi use ft df function call pass ani featur primit featuretool ha built mani new featur us use thi process doe automat creat new featur replac data scientist becaus still figur featur exampl goal predict whether client repay loan could look featur correl specifi outcom moreov domain knowledg use choos specif featur primit seed deep featur synthesi candid featur autom featur engin ha solv one problem creat anoth mani featur although difficult say befor fit model featur import like relev task want train model moreov mani featur lead poor model perform becaus less use featur drown import problem mani featur known curs dimension number featur increas dimens data grow becom difficult model learn map featur target fact amount data need model perform well scale exponenti number featur curs dimension combat featur reduct also known featur select process remov irrelev featur thi take mani form princip compon analysi pca selectkbest use featur import model auto encod use deep neural network howev featur reduct differ topic anoth articl know use featuretool creat numer featur mani tabl minim effort like mani topic machin learn autom featur engin featuretool complic concept built simpl idea use concept entityset entiti relationship featuretool perform deep featur synthesi creat new featur deep featur synthesi turn stack featur primit aggreg act across one mani relationship tabl transform function appli one column singl tabl build new featur multipl tabl futur articl show use thi techniqu real world problem home credit default risk competit current host kaggl stay tune post meantim read thi introduct get start competit hope use autom featur engin aid data scienc pipelin model onli good data give autom featur engin help make featur creation process effici inform featuretool includ advanc usag check onlin document see featuretool use practic read work featur lab compani behind open sourc librari alway welcom feedback construct critic reach twitter quick cheer stand ovat clap show much enjoy thi stori data scientist master student data scienc commun advoc share concept idea code\n",
            "understand machin learn big question mark thi blog post gradual increas awesomenessicitytm glu inspir video togeth friendli text sit relax video take time inspir continu next section fair enough howev find bottom thi articl earn well round knowledg passion thi new world go wa alway cool move paddl pong light combo street fighter ha alway revolv around programm function guess someth behav fun programm alway gift program often see googl epic game fail see glitch physic sometim even experienc human player regardless ha new talent teach comput play video game understand languag even identifi peopl thing thi tip iceberg new skill come old concept onli recent got process power exist outsid theori talk machin learn need come advanc algorithm anymor teach comput come advanc algorithm doe someth like even work algorithm realli written much sort bred use breed analog watch thi short video give excel commentari anim high level concept creat wow right crazi process even understand algorithm done one great visual wa wa written beat mario game human understand play side scroller identifi predict strategi result insan impress someth amaz thi idea right onli problem know machin learn know hook video game fortun elon musk alreadi provid non profit compani latter ye dozen line code hook ani want countless game task two good answer whi care firstli machin learn ml make comput thing never made comput befor want someth new new world ml secondli influenc world world influenc right signific compani invest ml alreadi see chang world thought leader warn let thi new age algorithm exist outsid public eye imagin corpor monolith control internet take arm scienc think christian heilmann said best hi talk ml concept use cool understand high level heck actual happen doe thi work want jump straight suggest skip thi section move next get start section motiv doer ml need video still tri grasp thi could even thing follow video perfect walk logic use classic ml problem handwrit pretti cool huh video show layer get simpler rather complic like function chew data smaller piec end abstract concept get hand dirti interact thi process thi site adam harley cool watch data go train model even watch neural network get train one classic real world exampl machin learn action iri data set present attend javafxpert overview machin learn learn use hi tool visual adjust back propag weight neuron neural network get watch train neural model even java buff present jim give thing machin learn pretti cool hour introduct ml concept includ info mani exampl abov concept excit readi einstein thi new era breakthrough happen everi day get start ton resourc avail recommend two approach thi approach understand machin learn algorithm math know thi way sound tough cool would realli get detail code thi stuff scratch want forc ml hold deep convers thi rout recommend tri brilliant org app alway great ani scienc lover take artifici neural network cours thi cours ha time limit help learn ml kill time line phone thi one cost money level combin abov simultan enrol andrew ng stanford cours machin learn week thi cours jim weaver recommend hi video abov also thi cours independ suggest jen looper everyon provid caveat thi cours tough show stopper whi go put collect certif say thi cours free onli pay certif want one two cours lot work everyon impress make becaus simpl make deep understand implement machin learn catapult success appli new world chang way interest write algorithm want use creat next breathtak websit app jump tensorflow crash cours tensorflow de facto open sourc softwar librari machin learn use countless way even javascript crash cours plenti inform avail cours rank found take cours style still luck learn nitti gritti ml order use today effici util ml servic mani way tech giant train model readi would still caution guarante data safe even offer servic ml quit attract use ml servic might best solut excit abl upload data amazon microsoft googl like think servic gateway drug advanc ml either way good get start say thank aforement peopl video inspir get start though still newb ml world happi light path embrac thi awe inspir age find ourselv imper reach connect peopl take learn thi craft without friendli face answer sound board anyth hard abl ask get respons game changer add add peopl mention abov friendli peopl friendli advic help see hope thi articl ha inspir around learn ml quick cheer stand ovat clap show much enjoy thi stori softwar consult adjunct professor publish author award win speaker mentor organ immatur nerd late full react nativ tech commun publish stori worth read develop design data scienc\n",
            "want learn appli artifici intellig lead practition silicon valley new york toronto learn insight artifici intellig fellow program compani work ai would like get involv insight ai fellow program feel free get touch recent gave talk reilli ai confer beij interest lesson learn world nlp wa lucki enough attend tutori deep reinforc learn deep rl scratch uniti technolog thought session led arthur juliani wa extrem inform want share big takeaway convers compani seen rise interest deep rl applic tool result parallel inner work applic deep rl alphago pictur abov often seem esoter hard understand thi post give overview core aspect field understood anyon mani visual slide talk new explan opinion mine anyth unclear reach deep rl field ha seen vast amount research interest includ learn play atari game beat pro player dota defeat go champion contrari mani classic deep learn problem often focu percept doe thi imag contain stop sign deep rl add dimens action influenc environ goal get dialog system exampl classic deep learn aim learn right respons given queri hand deep reinforc learn focus right sequenc sentenc lead posit outcom exampl happi custom thi make deep rl particularli attract task requir plan adapt manufactur self drive howev industri applic trail behind rapidli advanc result come research commun major reason deep rl often requir agent experi million time befor learn anyth use best way thi rapidli use simul environ thi tutori use uniti creat environ train agent thi workshop led arthur juliani leon chen goal wa get everi particip success train multipl deep rl algorithm hour tall order comprehens overview mani main algorithm power deep rl today complet set tutori arthur juliani wrote part seri start deep rl use best top human player go understand done first need understand simpl concept start much easier problem start slot machin let imagin face chest pick turn differ averag payout goal maxim total payout receiv fix number turn thi classic problem call multi arm bandit start crux problem balanc explor help us learn state good exploit use know pick best slot machin util valu function map action estim reward call q function first initi q valu equal valu updat q valu action pick chest base good payout wa choos thi action thi allow us learn good valu function approxim q function use neural network start veri shallow one learn probabl distribut use softmax potenti chest valu function tell us good estim action polici function determin action end take intuit might want use polici pick action highest q valu thi perform poorli practic q estim veri wrong start befor gather enough experi trial error thi whi need add mechan polici encourag explor one way use epsilon greedi consist take random action probabl epsilon start epsilon close alway choos random action lower epsilon go along learn chest good eventu learn chest best practic might want take subtl approach either take action think best random action popular method boltzmann explor adjust probabl base current estim good chest ad random factor ad differ state previou exampl wa world alway state wait pick chest front us real word problem consist mani differ state add environ next background behind chest altern color turn chang averag valu chest thi mean need learn q function depend onli action chest pick state color background thi version problem call contextu multi arm bandit surprisingli use approach befor onli thing need add extra dens layer neural network take input vector repres current state world learn consequ action anoth key factor make current problem simpler environ maze depict abov action take impact state world move thi grid might receiv reward might receiv noth next turn differ state thi final introduc need plan first defin q function immedi reward current state plu discount reward expect take futur action thi solut work q estim state accur learn good estim use method call tempor differ td learn learn good q function idea onli look limit number step futur td exampl onli use next state evalu reward surprisingli use td look current state estim reward next turn get great result structur network need go one forward step befor receiv error use thi error back propag gradient like tradit deep learn updat valu estim introduc mont carlo anoth method estim eventu success action mont carlo estim thi consist play entir episod current polici reach end success reach green block failur reach red block imag abov use result updat valu estim travers state thi allow us propag valu effici one batch end episod instead everi time make move cost introduc nois estim sinc attribut veri distant reward world rare discret previou method use neural network approxim valu estim map discret number state action valu maze exampl state squar action move adjac direct thi environ tri learn balanc ball dimension paddl decid time step whether want tilt paddl left right state space becom continu angl paddl posit ball good news still use neural network approxim thi function note polici vs polici learn method use previous polici method mean gener data ani strategi use epsilon greedi exampl learn polici method onli learn action taken follow polici rememb polici method use determin action take thi constrain learn process explor strategi built polici allow us tie result directli reason enabl us learn effici approach use call polici gradient polici method previous first learn valu function q action state build polici top vanilla polici gradient still use mont carlo estim learn polici directli loss function increas probabl choos reward action sinc learn polici use method epsilon greedi includ random choic get agent explor environ way encourag explor use method call entropi regular push probabl estim wider thu encourag us make riskier choic explor space leverag deep learn represent practic mani state art rl method requir learn polici valu estim way thi deep learn two separ output backbon neural network make easier neural network learn good represent one method thi advantag actor critic c learn polici directli polici gradient defin abov learn valu function use someth call advantag instead updat valu function base reward updat base advantag measur much better wors action wa previou valu function estim thi help make learn stabl compar simpl q learn vanilla polici gradient learn directli screen addit advantag use deep learn method deep neural network excel percept task human play game inform receiv list state imag usual screen board surround environ imag base learn combin convolut neural network cnn rl thi environ pass raw imag instead featur add layer cnn architectur without chang anyth els even inspect activ see network pick determin valu polici exampl see network use current score distant obstacl estim valu current state focus nearbi obstacl determin action neat side note toy around provid implement found visual learn veri sensit hyperparamet chang discount rate slightli exampl complet prevent neural network learn even toy applic thi wide known problem interest see first hand nuanc action far play environ continu discret state space howev everi environ studi discret action space could move one four direct tilt paddl left right ideal applic self drive car would like learn continu action turn steer wheel degre thi environ call ball world choos tilt paddl ani valu axe thi give us control perform action make action space much larger approach thi approxim potenti choic gaussian distribut learn probabl distribut potenti action learn mean varianc gaussian distribut polici sampl distribut simpl theori next step brave concept separ algorithm describ abov state art approach interest see conceptu best robot game play algorithm far away one explor thi overview hope thi ha inform fun look dive deeper theori rl give arthur post read dive deeper follow david silver ucl cours look learn project insight work compani pleas check us reach want learn appli artifici intellig lead practition silicon valley new york toronto learn insight artifici intellig fellow program compani work ai would like get involv insight ai fellow program feel free get touch quick cheer stand ovat clap show much enjoy thi stori ai lead insight ai insight fellow program bridg career data\n",
            "advent power versatil deep learn framework recent year ha made possibl implement convolut layer deep learn model extrem simpl task often achiev singl line code howev understand convolut especi first time often feel bit unnerv term like kernel filter channel stack onto yet convolut concept fascinatingli power highli extens thi post break mechan convolut oper step step relat standard fulli connect network explor build strong visual hierarchi make power featur extractor imag convolut fairli simpl oper heart start kernel simpli small matrix weight thi kernel slide input data perform elementwis multipl part input current sum result singl output pixel kernel repeat thi process everi locat slide convert matrix featur yet anoth matrix featur output featur essenti weight sum weight valu kernel input featur locat roughli locat output pixel input layer whether input featur fall within thi roughli locat get determin directli whether area kernel produc output thi mean size kernel directli determin mani input featur get combin product new output featur thi pretti stark contrast fulli connect layer abov exampl input featur output featur thi standard fulli connect layer weight matrix paramet everi output featur weight sum everi singl input featur convolut allow us thi transform onli paramet output featur instead look everi input featur onli get look input featur come roughli locat take note thi critic later discuss befor move definit worth look two techniqu commonplac convolut layer pad stride pad doe someth pretti clever solv thi pad edg extra fake pixel usual valu henc oft use term zero pad thi way kernel slide allow origin edg pixel center extend fake pixel beyond edg produc output size input idea stride skip slide locat kernel stride mean pick slide pixel apart basic everi singl slide act standard convolut stride mean pick slide pixel apart skip everi slide process downsiz roughli factor stride mean skip everi slide downsiz roughli factor modern network resnet architectur entir forgo pool layer intern layer favor stride convolut need reduc output size cours diagram abov onli deal case imag ha singl input channel practic input imag channel number onli increas deeper go network pretti easi think channel gener view imag whole emphasis aspect de emphasis thi key distinct term come handi wherea channel case term filter kernel interchang gener case actual pretti differ filter actual happen collect kernel one kernel everi singl input channel layer kernel uniqu filter convolut layer produc one onli one output channel like kernel filter slide respect input channel produc process version kernel may stronger weight give emphasi certain input channel eg filter may red kernel channel stronger weight henc respond differ red channel featur per channel process version sum togeth form one channel kernel filter produc one version channel filter whole produc one overal output channel final bia term way bia term work output filter ha one bia term bia get ad output channel far produc final output channel singl filter case case ani number filter ident filter process input differ set kernel scalar bia process describ abov produc singl output channel concaten togeth produc overal output number output channel number filter nonlinear usual appli befor pass thi input anoth convolut layer repeat thi process even mechan convolut layer still hard relat back standard feed forward network still explain whi convolut scale work much better imag data suppos input want transform grid use feedforward network reshap input vector length pass dens connect layer input output one could visual weight matrix w layer although convolut kernel oper may seem bit strang first still linear transform equival transform matrix use kernel k size reshap input get output equival transform matrix would note abov matrix equival transform matrix actual oper usual implement veri differ matrix multipl convolut whole still linear transform time also dramat differ kind transform matrix element paramet themselv reus sever time output node onli get see select number input one insid kernel interact ani input weight set use see convolut oper hard prior weight matrix thi context prior mean predefin network paramet exampl use pretrain model imag classif use pretrain network paramet prior featur extractor final dens connect layer sens direct intuit whi effici compar altern transfer learn effici order magnitud compar random initi becaus onli realli need optim paramet final fulli connect layer mean fantast perform onli dozen imag per class need optim paramet becaus set zero stay way rest convert share paramet result onli actual paramet optim thi effici matter becaus move input mnist real world imag input dens layer attempt halv input input would still requir billion paramet comparison entireti resnet ha million paramet fix paramet tie paramet increas effici unlik transfer learn case know prior good becaus work larg gener set imag know thi ani good answer lie featur combin prior lead paramet learn earli thi articl discuss backpropag come way classif node network kernel interest task learn weight produc featur onli set local input addit becaus kernel appli across entir imag featur kernel learn must gener enough come ani part imag thi ani kind data eg categor data app instal thi would disast becaus number app instal app type column next mean ani local share featur common app instal date time use sure four may underli higher level featur eg app peopl want found give us reason believ paramet first two exactli paramet latter two four could ani consist order still valid pixel howev alway appear consist order nearbi pixel influenc pixel e g nearbi pixel red pretti like pixel also red deviat interest anomali could convert featur thi detect compar pixel neighbor pixel local thi idea realli lot earlier comput vision featur extract method base around instanc edg detect one use sobel edg detect filter kernel fix paramet oper like standard one channel convolut non edg contain grid eg background sky pixel valu overal output kernel point grid vertic edg differ pixel left right edg kernel comput differ non zero activ reveal edg kernel onli work onli grid time detect anomali local scale yet appli across entir imag enough detect certain featur global scale anywher imag key differ make deep learn ask thi question use kernel learnt earli layer oper raw pixel could reason expect featur detector fairli low level featur like edg line etc entir branch deep learn research focus make neural network model interpret one power tool come featur visual use optim idea core simpl optim imag usual initi random nois activ filter strongli possibl thi doe make intuit sens optim imag complet fill edg strong evid filter look activ use thi peek learnt filter result stun one import thing notic convolv imag still imag output small grid pixel top left imag still top left run anoth convolut layer top anoth two left extract deeper featur visual yet howev deep featur detector get without ani chang still oper veri small patch imag matter deep detector detect face grid thi idea recept field come essenti design choic ani cnn architectur input size grow smaller smaller start end network number channel grow deeper thi mention earlier often done stride pool layer local determin input previou layer output get see recept field determin area origin input entir network output get see idea stride convolut onli process slide fix distanc apart skip one middl differ point view onli keep output fix distanc apart remov rest appli nonlinear output per usual stack anoth new convolut layer top thi thing get interest even appli kernel size local area output stride convolut kernel would larger effect recept field thi becaus output stride layer still doe repres imag much crop resiz onli thing singl pixel output repres larger area whose pixel discard rough locat origin input next layer kernel oper output oper pixel collect larger area note familiar dilat convolut note abov dilat convolut method increas recept field dilat convolut singl layer thi take place regular convolut follow stride convolut nonlinear inbetween thi expans recept field allow convolut layer combin low level featur line edg higher level featur curv textur see mix layer follow pool stride layer network continu creat detector even higher level featur part pattern see mix repeat reduct imag size across network result th block convolut input size compar input thi point singl pixel repres grid pixel huge compar earlier layer activ meant detect edg activ tini grid one veri high level featur bird network whole progress small number filter case googlenet detect low level featur veri larg number filter final convolut look extrem specif high level featur follow final pool layer collaps grid singl pixel channel featur detector recept field equival entir imag compar standard feedforward network would done output realli noth short awe inspir standard feedforward network would produc abstract featur vector combin everi singl pixel imag requir intract amount data train cnn prior impos start learn veri low level featur detector across layer recept field expand learn combin low level featur progress higher level featur abstract combin everi singl pixel rather strong visual hierarchi concept detect low level featur use detect higher level featur progress visual hierarchi eventu abl detect entir visual concept face bird tree etc make power yet effici imag data visual hierarchi cnn build pretti reason assum vision system similar human realli great real world imag also fail way strongli suggest vision system entir human like major problem adversari exampl exampl specif modifi fool model adversari exampl would non issu onli tamper one caus model fail one even human would notic problem model suscept attack sampl onli tamper ever slightli would clearli fool ani human thi open door model silent fail pretti danger wide rang applic self drive car healthcar robust adversari attack current highli activ area research subject mani paper even competit solut certainli improv cnn architectur becom safer reliabl cnn model allow comput vision scale simpl applic power sophist product servic rang face detect photo galleri make better medic diagnos might key method comput vision go forward new breakthrough might around corner regardless one thing sure noth short amaz heart mani present day innov applic certainli worth deepli understand hope enjoy thi articl like stay connect find twitter question comment welcom find use learn process well quick cheer stand ovat clap show much enjoy thi stori curiou programm tinker around python deep learn share concept idea code\n",
            "ongo debat whether design write code wherev fall thi issu peopl would agre design know code thi help design understand constraint empath develop also allow design think outsid pixel perfect box problem solv reason design know machin learn put simpli machin learn field studi give comput abil learn without explicitli program arthur samuel even though arthur samuel coin term fifti year ago onli recent seen excit applic machin learn digit assist autonom drive spam free email exist thank machin learn past decad new algorithm better hardwar data made machin learn order magnitud effect onli past year compani like googl amazon appl made power machin learn tool avail develop best time learn machin learn appli product build sinc machin learn access ever befor design today opportun think machin learn appli improv product design abl talk softwar develop possibl prepar outcom expect exampl applic serv inspir convers machin learn help creat user centric product person experi individu use thi allow us improv thing like recommend search result notif ad machin learn effect find abnorm content credit card compani use thi detect fraud email provid use thi detect spam social media compani use thi detect thing like hate speech machin learn ha enabl comput begin understand thing say natur languag process thing see comput vision thi allow siri understand siri set remind googl photo creat album dog facebook describ photo visual impair machin learn also help understand user group thi insight use look analyt group group basi differ featur evalu across group roll onli particular group user machin learn allow us make predict user might behav next know thi help prepar user next action exampl predict content user plan view preload content immedi readi want depend applic data avail differ type machin learn algorithm choos briefli cover follow supervis learn allow us make predict use correctli label data label data group exampl ha inform tag output exampl photo associ hashtag hous featur eq number bedroom locat price use supervis learn fit line label data either split data categori repres trend data use thi line abl make predict new data exampl look new photo predict hashtag look new hous featur predict price output tri predict list tag valu call classif output tri predict number call regress unsupervis learn help unlabel data exactli sure output like imag hashtag hous price meaning instead identifi pattern among unlabel data exampl identifi relat item e commerc websit recommend item someon base made similar purchas pattern group call cluster pattern rule e q thi call associ reinforc learn use exist data set instead creat agent collect data trial error environ reinforc reward exampl agent learn play mario receiv posit reward collect coin neg reward walk goomba reinforc learn inspir way human learn ha turn effect way teach comput specif reinforc ha effect train comput play game like go dota understand problem tri solv avail data constrain type machin learn use e q identifi object imag supervis learn requir label data set imag howev constraint fruit creativ case set collect data alreadi avail consid approach even though machin learn scienc come margin error import consid user experi might impact thi margin error exampl autonom car fail recogn surround peopl get hurt even though machin learn ha never access today still requir addit resourc develop time integr product thi make import think whether result impact justifi amount resourc need implement bare cover tip iceberg hope thi point feel comfort think machin learn appli product interest learn machin learn help resourc thank read chat twitter quick cheer stand ovat clap show much enjoy thi stori digit product design samueldrozdov com curat stori user experi usabl product design\n",
            "data scienc interview certainli easi know thi first hand particip individu interview phone screen appli competit internship last calendar year thi excit somewhat time veri pain process accumul plethora use resourc help prepar eventu pass data scienc interview long stori short decid sort bookmark note order deliv comprehens list data scienc resourc thi list side enough effect tool dispos next time prep big interview worth note mani resourc natur go gear toward entri level intern data scienc posit expertis lie keep mind enjoy gener resourc cover data scienc whole specif highli recommend check first two link regard data scienc interview question ebook coupl buck pocket answer themselv free quora favorit full coverag question practic right befor interview even data scientist escap dread algorithm code interview experi thi case time chanc ask work someth similar easi medium question leetcod hackerrank far languag goe compani let use whatev languag want person almost algorithm code java even though posit target python r programm recommend one thing break wallet invest crack code interview absolut live hype plan continu use year come onc interview know think problem code effect chanc move onto data scienc specif applic depend interview posit like abl choos python r tool choic sinc partial python resourc primarili focu effect use panda numpi data analysi data scienc interview typic complet without check knowledg sql thi done phone live code question like latter found difficulti level question vari good bit rang pain easi requir complex join obscur function good friend statist still crucial data scientist reflect interview mani interview begin see explain common statist probabl concept simpl concis term posit get experienc suspect thi happen less less tradit statist question begin take practic form b test scenario cover later post notic compil resourc section thi mistak machin learn complex field virtual guarante data scienc interview today way test thi guarante howev may come conceptu question regard cross valid bia varianc tradeoff may take form take home assign dataset attach seen sever time got prepar anyth specif check machin learn flashcard onli coupl buck far favorit way quiz ani conceptu ml stuff thi cover everi singl data scienc interview certainli uncommon interview atleast one section sole dedic product think often lend b test sort make sure familiar concept statist background necessari order prepar come time spare took free onlin cours udac overal wa pretti impress lastli want call post relat data scienc job interview read understand onli prepar expect well onli check one section thi one focu thi layer sit top technic skill applic overlook hope find resourc use dure next interview job search know truth glad save link somewher lastli thi post part ongo initi open sourc experi appli interview data scienc posit enjoy thi content sure follow stuff like thi interest receiv weekli rundown interest articl resourc focus data scienc machin learn artifici intellig subscrib self driven data scienc use form enjoy thi post feel free hit clap button interest post come make sure follow medium link write ship everi day thi month part day challeng thi articl wa origin publish conordewey com quick cheer stand ovat clap show much enjoy thi stori data scientist writer www conordewey com share concept idea code\n",
            "inform theori import field ha made signific contribut deep learn ai yet unknown mani inform theori seen sophist amalgam basic build block deep learn calculu probabl statist exampl concept ai come inform theori relat field earli th centuri scientist engin struggl question quantifi inform analyt way mathemat measur tell us inform content exampl consid two sentenc difficult tell second sentenc give us inform sinc also tell bruno big brown addit dog quantifi differ two sentenc mathemat measur tell us much inform second sentenc compar first scientist struggl question semant domain form data onli ad complex problem mathematician engin claud shannon came idea entropi chang world forev mark begin digit inform age shannon propos semant aspect data irrelev natur mean data matter come inform content instead quantifi inform term probabl distribut uncertainti shannon also introduc term bit humbl credit hi colleagu john tukey thi revolutionari idea onli laid foundat inform theori also open new avenu progress field like artifici intellig discuss four popular wide use must known inform theoret concept deep learn data scienc also call inform entropi shannon entropi entropi give measur uncertainti experi let consid two experi compar two experi exp easier predict outcom compar exp say exp inher uncertain unpredict exp thi uncertainti experi measur use entropi therefor inher uncertainti experi ha higher entropi lesser experi predict entropi probabl distribut experi use calcul entropi determinist experi complet predict say toss coin p h ha entropi zero experi complet random say roll fair dice least predict ha maximum uncertainti ha highest entropi among experi anoth way look entropi averag inform gain observ outcom random experi inform gain outcom experi defin function probabl occurr outcom rarer outcom inform gain observ exampl determinist experi alway know outcom new inform gain observ outcom henc entropi zero discret random variabl x possibl outcom state x x n entropi unit bit defin p x probabl th outcom x cross entropi use compar two probabl distribut tell us similar two distribut cross entropi two probabl distribut p q defin set outcom given mutual inform measur mutual depend two probabl distribut random variabl tell us much inform one variabl carri anoth variabl mutual inform captur depend random variabl gener vanilla correl coeffici captur onli linear relationship mutual inform two discret random variabl x defin p x joint probabl distribut x p x p margin probabl distribut x respect also call rel entropi kl diverg anoth measur find similar two probabl distribut measur much one distribut diverg suppos data true distribut underli p know thi p choos new distribut q approxim thi data sinc q approxim abl approxim data good p inform loss occur thi inform loss given kl diverg kl diverg p q tell us much inform lose tri approxim data given p q kl diverg probabl distribut q anoth probabl distribut p defin kl diverg commonli use unsupervis machin learn techniqu variat autoencod inform theori wa origin formul mathematician electr engin claud shannon hi semin paper mathemat theori commun note term experi random variabl ai machin learn deep learn data scienc use loos abov technic differ mean case like articl follow abhishek parbhakar articl relat ai philosophi econom quick cheer stand ovat clap show much enjoy thi stori find equilibria among ai philosophi econom share concept idea code\n",
            "past month interview variou compani like googl deepmind wadhwani institut ai microsoft ola fractal analyt primarili role data scientist softwar engin research engin process onli get opportun interact mani great mind also peek along sens peopl realli look interview someon believ thi knowledg befor could avoid mani mistak prepar much better manner motiv behind thi post abl help someon bag dream place work thi post aros discuss one junior lack realli fulfil job opportun offer campu placement peopl work ai also wa prepar notic peopl use lot resourc per experi past month realis one away minim one role ai go mention end post begin get notic k interview provid list compani start appli follow ace interview base whatev experi add section strive work conclud minim resourc need prepar note peopl sit campu placement two thing like add firstli go say except last one mayb go relev placement thi second point mention befor opportun campu mostli softwar engin role intersect ai thi post specif meant peopl want work solv interest problem use ai also want add clear interview guess essenc failur greatest teacher thing mention may use thing way know might end make case stronger honest thi step import one make campu placement tough exhaust get recruit actual go profil among plethora applic get contact insid organis place referr would make quit easi gener thi part sub divid three key step regulatori prepar well regulatori prepar mean linkedin profil github profil portfolio websit well polish cv firstli cv realli neat concis follow thi guid udac clean cv resum revamp ha everyth intend say use refer guid cv templat built format overleaf quit nice person use deedi resum preview seen lot content fit one page howev realli need format link abov would work directli instead find modifi multi page format next import thing mention github profil lot peopl underestim potenti thi becaus unlik linkedin view profil option peopl go github becaus onli way valid mention cv given lot nois today peopl associ kind buzzword profil especi data scienc open sourc ha big role play major tool implement variou algorithm list learn resourc open sourc discuss benefit get involv open sourc one start scratch earlier post bare minimum creat github account alreadi one creat repositori project done add document clear instruct run code add document file mention role function mean paramet proper format e g pep python along script autom previou step option move third step peopl lack portfolio websit demonstr experi person project make portfolio indic realli seriou get field add lot point authent factor also gener space constraint cv tend miss lot detail use portfolio realli delv deep detail want highli recommend includ sort visualis demonstr project idea realli easi creat one lot free platform drag drop featur make process realli painless person use weebli wide use tool better refer begin lot awesom one refer deshraj yadav person websit begin make mine final lot recruit start nowaday start use linkedin go platform hire lot good job get post apart recruit peopl work influenti posit quit activ well grab attent good chanc get apart maintain clean profil necessari peopl connect import part linkedin search tool show must relev keyword interspers profil took lot iter evalu final decent one also definit ask peopl work endors skill add recommend talk experi work thi increas chanc actual get notic point toward udac guid linkedin github profil thi might seem like lot rememb need singl day even week month process never end set everyth first would definit take effort onc keep updat regularli event around keep happen onli find quit easi also abl talk anywher anytim without explicitli prepar becaus becom awar b stay authent seen lot peopl thi mistak present themselv per differ job profil accord alway better first decid actual interest would happi search relev opportun way round fact demand ai talent surpass suppli give thi opportun spend time regulatori prepar mention abov would give around perspect help make thi decis easier also need prepar answer variou kind question get ask dure interview would come natur talk someth realli care c network onc done figur b network actual help get talk peopl miss hear mani opportun might good shot import keep connect new peopl day physic linkedin upon compound mani day larg strong network network messag peopl place referr wa start thi mistak way often stumbl upon thi excel articl mark meloon talk import build real connect peopl offer help first anoth import step network get content exampl good someth blog share blog facebook linkedin onli doe thi help help well onc good enough network visibl increas multi fold never know one person network like comment post may help reach much broader audienc includ peopl might look someon expertis present thi list alphabet order avoid misinterpret ani specif prefer howev place one person recommend thi recommend base either follow mission statement peopl person interact scope learn pure base nd rd factor interview begin moment enter room lot thing happen moment time ask introduc bodi languag fact smile greet play big role especi interview start cultur fit someth extrem care need understand much interview stranger stranger probabl nervou import view interview convers interview look mutual fit look awesom place work interview look awesom person like work make sure feel good take charg make initi moment convers pleasant easiest way know make happen smile mostli two type interview one interview ha come come prepar set question go ask irrespect profil second interview base cv start second one thi kind interview gener begin tell bit thi point thing big talk gpa colleg talk project detail ideal statement minut two long give good idea till restrict academ talk hobbi like read book play sport medit etc basic anyth contribut defin interview take someth talk cue hi next question technic part interview begin motiv thi kind interview realli check whether whatev written cv true would lot question could done differ x wa use instead would happen thi point import know kind trade usual made dure implement e g interview say use complex model would given better result might say actual less data work would lead overfit one interview wa given case studi work involv design algorithm real world use case notic onc given green flag talk project interview realli like talk follow flow problem previou approach approach result intuit kind interview realli test basic knowledg expect question hard would definit scratch everi bit basic mainli base around linear algebra probabl statist optimis machin learn deep learn resourc mention minim resourc need prepar section suffic make sure miss one bit among catch amount time take answer question sinc cover basic expect answer almost instantli prepar accordingli throughout process import confid honest know know question certain idea say upfront rather make aah um sound concept realli import struggl answer interview would gener depend initi part happi give hint guid toward right solut big plu manag pick hint arriv correct solut tri get nervou best way avoid smile come conclus interview interview would ask ani question realli easi think interview done say noth ask know mani peopl got reject becaus fail thi last question mention befor onli interview also look mutual fit compani quit obviou realli want join place must mani question regard work cultur kind role see simpl curiou person interview alway someth learn everyth around make sure leav interview impress truli interest part team final question start ask interview feedback might want improv thi ha help tremend still rememb everi feedback gotten incorpor daili life base experi honest compet truli care compani interview right mindset tick right box get congratulatori mail soon live era full opportun appli anyth love need strive becom best find way monetis gari vaynerchuk follow alreadi say thi great time work ai truli passion much ai empow mani peopl alway repres keep nag problem surround us never time common peopl like us actual someth problem rather complain jeffrey hammerbach founder cloudera famous said much ai ever imagin mani extrem challeng problem requir incred smart peopl like put head solv make mani live better time let go cool would look good think choos wise ani data scienc interview compris question mostli subset follow four categori comput scienc math statist machin learn familiar math behind deep learn consid go last post resourc understand howev comfort found chapter deep learn book enough prepar revis theoret question dure interview prepar summari chapter refer tri even explain concept found challeng understand first case go entir chapter alreadi done cours probabl comfort answer numer well stat cover topic enough rang question vari depend type posit appli tradit machin learn base interview want check basic knowledg ml complet ani one follow cours machin learn andrew ng cs machin learn cours caltech professor yaser abu mostafa import topic supervis learn classif regress svm decis tree random forest logist regress multi layer perceptron paramet estim bay decis rule unsupervis learn k mean cluster gaussian mixtur model dimension reduct pca appli advanc posit high chanc might question deep learn case veri comfort convolut neural network cnn depend upon work recurr neural network rnn variant comfort must know fundament idea behind deep learn cnn rnn actual work kind architectur propos ha motiv behind architectur chang shortcut thi either understand put enough time understand cnn recommend resourc stanford cs n cs n rnn found thi neural network class hugo larochel realli enlighten refer thi quick refresh udac come aid figur udac realli import place ml practition lot place work reinforc learn rl india experienc rl one thing add thi post sometim futur get place campu long journey self realis realis thi ha anoth long post extrem grate valu thought hope thi post find way use help way prepar next data scienc interview better request realli think talk strive work veri thank friend iit guwahati help feedback especi ameya godbol kothap vignesh prabal jain major mention like view interview convers seek feedback interview aros multipl discuss prabal ha advis constantli improv interview skill thi stori publish noteworthi thousand come everi day learn peopl idea shape product love follow public see product design stori featur journal team quick cheer stand ovat clap show much enjoy thi stori ai fanat math lover dreamer offici journal blog\n"
          ]
        }
      ],
      "source": [
        "for line in preprocessed_articles[:10]:\n",
        "    print(line)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "38uci1Z1_6ar",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "6d50577ddd8a32cad9d001554abd3ebc",
          "grade": false,
          "grade_id": "cell-6fe6a2050f50f831",
          "locked": true,
          "schema_version": 3,
          "solution": false
        }
      },
      "source": [
        "## Exercise 1: Fitting CountVectorizer\n",
        "<b><div style=\"text-align: right\">[POINTS: 2]</div></b>\n",
        "\n",
        "In this assignment, you will implement the TF-IDF vectorizer using the `TfidfTransformer`. For this, you'll first need to get the count matrix using the `CountVectorizer`.\n",
        "\n",
        "**Tasks:**\n",
        "\n",
        "1. Import the `CountVectorizer` from sklearn.\n",
        "2. Instantiate the object of the `CountVectorizer'class as `count_vectorizer`.\n",
        "3. Fit the `count_vectorizer` on the `preprocessed_articles` and transform it into the feature matrix `X_count`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "id": "guJjfIg48W9o",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "2c24de81413757ed677737b6a8dc8a03",
          "grade": false,
          "grade_id": "cell-4886b676523d814e",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": [
          "Ex-1-Task-1"
        ]
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "count_vectorizer = None\n",
        "X_count = None\n",
        "\n",
        "### Ex-1-Task-1\n",
        "### BEGIN SOLUTION\n",
        "# YOUR CODE HERE\n",
        "count_vectorizer = CountVectorizer()\n",
        "X_count = count_vectorizer.fit_transform(preprocessed_articles)\n",
        "### END SOLUTION\n",
        "\n",
        "if (type(X_count) != np.ndarray):\n",
        "    X_count = X_count.toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "iCZXRqleu41m",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "be3c672e9abeea80b2ebb5d594f2ea7e",
          "grade": true,
          "grade_id": "cell-2a50ded563997b28",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [
          "Ex-1-Task-1"
        ]
      },
      "outputs": [],
      "source": [
        "### INTENTIONALLY LEFT BLANK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDivXQlcI_XS"
      },
      "source": [
        "Let's have a look at 10 words from our vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AG6Yh3liI_Hb",
        "outputId": "09057a2a-6181-4914-9305-fc5cc62b62e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['oh', 'headlin', 'blare', 'chatbot', 'next', 'big', 'thing', 'hope', 'sky', 'high']\n"
          ]
        }
      ],
      "source": [
        "print(list(count_vectorizer.vocabulary_.keys())[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhQ_kctN-5Qw"
      },
      "source": [
        "## Exercise 2: Fitting TfidfTransformer\n",
        "<b><div style=\"text-align: right\">[POINTS: 3]</div></b>\n",
        "\n",
        "\n",
        "Till now, you have implemented the TF-IDF using either the `TfidfVectorizer` or from scratch. Here you will learn another way to implement it using the [`TfidfTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html). `TfidfTransformer` uses a count matrix containing the count of words in the documents, like the one calculated by the `CountVectorizer`, to calculate the TF-IDF values.\n",
        "Using `TfidfTransformer` is similar to using other methods from sklearn. First, you instantiate the `TfidfTransformer` class. Then you use the `fit` method to compute the IDF values for the words in the vocabulary. Finally, you use the `transform` method to compute the TF-IDF value and transform the count matrix into a feature matrix containing TF-IDF values.\n",
        "\n",
        "However, in this assignment, you won't have to create a TF-IDF feature matrix from the documents as you won't be applying any machine learning model. Instead, you will justcompute the IDF values for the words in the vocabulary and use them to compute the TF-IDF values for a new test document.\n",
        "\n",
        "**Tasks:**\n",
        "\n",
        "1. Import the `TfidfTransformer` from sklearn.\n",
        "2. Instantiate the object of the `TfidfTransformer'class as `tfidf_transformer`.\n",
        "3. Fit the `tfidf_transformer` on the feature matrix `X_count`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "deletable": false,
        "id": "kcMhQeh8TKY5",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "52c7f04570542edd10f313731e349781",
          "grade": false,
          "grade_id": "cell-ff312a2a4d4af4b3",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "outputId": "1ef351e5-f336-4bb2-9dd8-eecd02d8430c",
        "tags": [
          "Ex-2-Task-1"
        ]
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfTransformer()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfTransformer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfTransformer</label><div class=\"sk-toggleable__content\"><pre>TfidfTransformer()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "tfidf_transformer = None\n",
        "\n",
        "### Ex-2-Task-1\n",
        "### BEGIN SOLUTION\n",
        "# YOUR CODE HERE\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "tfidf_transformer.fit(X_count)\n",
        "### END SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "pEg2jIZp9RFW",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "e1590357b959868cfbfa86c02b817419",
          "grade": true,
          "grade_id": "cell-66055e6cedb45212",
          "locked": true,
          "points": 3,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [
          "Ex-2-Task-1"
        ]
      },
      "outputs": [],
      "source": [
        "### INTENTIONALLY LEFT BLANK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOmE_DXVoY-m"
      },
      "source": [
        "## Exercise 3: Scraping a webpage containing a blog post\n",
        "<b><div style=\"text-align: right\">[POINTS: 1]</div></b>\n",
        "\n",
        "Now that you have learned the vocabulary and their IDF scores using the medium articles dataset, we can use them to compute the TF-IDF scores of new text documents. Let's start with scraping a webpage containing a blog post. You will scrape the page [AI to the Rescue: COVID-19 Recovery](https://insights.fusemachines.com/ai-to-the-rescue-the-covid-19-recovery/) from [Insights- Fusemachines](https://insights.fusemachines.com/).\n",
        "\n",
        "**Tasks:**\n",
        "\n",
        "1. Import the `requests` module.\n",
        "2. Extract the site stored in the `URL` variable and store it in the variable `response`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "id": "Tj-Ms5FaHXv_",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "8f58cdf30f86b6b7387c1902629bee19",
          "grade": false,
          "grade_id": "cell-6ea60cfa6c2846be",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "outputId": "89338154-1e96-48ca-fe23-49a9eef12fe2",
        "tags": [
          "Ex-3-Task-1"
        ]
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200\n"
          ]
        }
      ],
      "source": [
        "URL = 'https://insights.fusemachines.com/ai-to-the-rescue-the-covid-19-recovery/'\n",
        "response = None\n",
        "\n",
        "### Ex-3-Task-1\n",
        "### BEGIN SOLUTION\n",
        "# YOUR CODE HERE\n",
        "import requests\n",
        "response = requests.get(URL)\n",
        "# raise NotImplementedError()\n",
        "### END SOLUTION\n",
        "\n",
        "print(response.status_code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "IO1ItQGsCAz2",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "c2d8f9f74f96e9b906501bf77e5d0b56",
          "grade": true,
          "grade_id": "cell-b14fa4e1ad023001",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [
          "Ex-3-Task-1"
        ]
      },
      "outputs": [],
      "source": [
        "### INTENTIONALLY LEFT BLANK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bv1-roB06iKH"
      },
      "source": [
        "## Exercise 4: Extracting the blog post\n",
        "<b><div style=\"text-align: right\">[POINTS: 3]</div></b>\n",
        "\n",
        "The entire HTML page is now stored in the `response` variable. Now, you will use `Beautiful Soup` to extract the content from the page. All the contents of the blog are inside the \\<p> tag.\n",
        "\n",
        "**Tasks:**\n",
        "\n",
        "1. Import `BeautifulSoup` class from `bs4` module.\n",
        "\n",
        "2. Create an object, `soup` of the `BeautifulSoup` class, and initialize it with two parameters, the content of our response and the `html.parser`.\n",
        "\n",
        "3. Find all the \\<p> tags using the `find_all` method of the object `soup` and store it in the variable `paras`.\n",
        "\n",
        "4. Extract the text from all the \\<p> tags using the `get_text` method and store them in the list `texts`. $[$Hint: Use a for loop to access each element of the list `paras` and use the `get_text` method of each element to extract the text content of the element$]$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "id": "76fVlS2E81c6",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "31d0d309efae2eed7d8033c6a97a1e19",
          "grade": false,
          "grade_id": "cell-1324c76b451779ca",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "outputId": "7120965f-5593-41ea-f00f-82b1bdabbdc5",
        "tags": [
          "Ex-4-Task-1"
        ]
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Even before the COVID-19 crisis struck and plunged the world into recession, people and businesses were adopting AI to take on all varieties of tasks. With most of the global workforce working from home, AI has become all the more important and will continue to affect and shape the way the world recovers. During past economic recessions, 14% of companies were able to increase sales by adopting digital solutions.  Consumer behavior  It’s no surprise consumers are turning online, a behavior that began long before the pandemic. Consumers are operating nearly entirely online, indeed, they have no other choice. Under lockdown, today’s consumers are choosing reduced contact means of getting goods and services. This is especially true for younger and higher-income consumers. In today’s digitized world, one can order food, take classes, and work all without leaving home. This trend is unlikely to disappear once the crisis is over.  Automate repetitive tasks so people can focus on creative tasks Businesses now are taking advantage of this unique time to capitalize on the online market and consumers’ willingness to to partake in it and this will set the tone for how businesses function in the post-COVID-19 market. Leveraging AI, now and always, provides competitive advantage as it has the power to automate repetitive and time consuming tasks previously only done by humans. With AI, humans can better spend their time on more demanding and dynamic tasks and channel their creativity where it’s most needed.  Boost supply chain efficiency and reduce need for human interaction So what exactly is the benefit of adopting AI and why will it help businesses recover post COVID-19? Data is only helpful if it can be processed at scale. Al makes this possible. Today’s supply chains have reached a level of complexity that require machines to process and track all of the information. Leveraging data will help minimize the need for unnecessary human to human interaction.  Enhancing manufacturing  This level of data processing is especially helpful in the manufacturing and food industries. In manufacturing, data can be used in predictive maintenance which saves time and money, increases value, boost quality control with computer vision, and reduces error. AI will enable factories to operate 24/7 from more locations and with little extra cost.  Boosting efficiency in the food industry AI also helps save time in the food industry by predicting trends, preferences, and planning delivery routes. Not only this, it helps forecast demands and strengthens relationships with customers through engaging chatbots. These assets have been especially useful during COVID-19 lockdown when people have relied on online ways of getting food. Enhance decision making, insights, and fraud detection  Leveraging AI and data will also help companies’ decision making capabilities in both private and public organizations. Companies may start to rely entirely on AI to make decisions on behalf of humans or may be leveraged to provide a way for companies to trace decision making back to humans. AI can also generate insights based on patterns and trends. The processing of large quantities of data will also help boost security and detect anomalous behavior and fraud early on.  Unprecedented personalization, customer engagement, and loyalty AI helps companies understand and engage with their customers on unprecedented levels. AI makes this possible, opening doors for companies to understand their customers’ behavior (segmenting by demographics and purchasing patterns) and make better recommendations to them, ultimately fostering increased customer loyalty. This type of advanced data analytics and machine learning allows companies to personalize their services in ways unthinkable before AI. A better understanding of customers allows companies to brand and market themselves in more effective and relevant ways.  Successful remote working Companies already powered by AI will have a better time transitioning to remote working environments and will create more opportunities for on demand labor. These companies are also better able to simulate normal work environments as their platforms are already AI enabled. Businesses across the globe may realize these advantages, adopting AI and increasing their remote workforce in the future. These companies will be better poised to continue remote working if they need to and maintain clear and consistent communication over space and time.  Holistic financial sector  AI will also enhance the financial sector. With greater security, online payments and spending will help us interact with the digital economy on a daily basis. Financial institutions will become more than banks to their customers and will embrace more holistic solutions and services helping them ensure transparency and assurance to customers.   Providing better healthcare services A most relevant field that will certainly embrace AI is healthcare. AI has been leveraged in a multitude of ways during the pandemic, from scanning lung X-rays and predicting diagnosis by evaluating worsening symptoms. This is surely only the beginning. AI may help forecast future epidemics and help patients self-diagnose. It will also enable biometrics. By processing large amounts of medical data, AI will help healthcare organizations build and compare statistical epidemiological models. Already we’re seeing hospitals forgoing inefficient procedures to aid their frontline workers. The effects of COVID-19 on business and society will be large and long lasting, ultimately contributing to a more resilient and dynamic way of life. With its power to leverage data in unprecedented and powerful ways, AI promises to not only transform business but also help businesses recover from COVID-19. AI has come to the rescue, but it’s up to us to heed its call.   © Copyright 2023 Fusemachines Inc.  |  All rights reserved   Privacy Policy | EULA     © Copyright 2023 Fusemachines Inc.  |  All rights reserved\n"
          ]
        }
      ],
      "source": [
        "soup = None\n",
        "paras = None\n",
        "texts = []\n",
        "\n",
        "### Ex-4-Task-1\n",
        "### BEGIN SOLUTION\n",
        "# YOUR CODE HERE\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Create a BeautifulSoup object with the HTML content from the response\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "paras = soup.find_all('p')\n",
        "\n",
        "texts = []\n",
        "for para in paras:\n",
        "    text = para.get_text()\n",
        "    texts.append(text)\n",
        "# raise NotImplementedError()\n",
        "### END SOLUTION\n",
        "\n",
        "article = \" \".join(texts)\n",
        "print(article)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "XVN3-rQaHcCe",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a7482ca2d3c596e6bd7bfebf4a5f0d44",
          "grade": true,
          "grade_id": "cell-679908f4ecda8803",
          "locked": true,
          "points": 3,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [
          "Ex-4-Task-1"
        ]
      },
      "outputs": [],
      "source": [
        "### INTENTIONALLY LEFT BLANK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voJ7oqKlzRkF"
      },
      "source": [
        "## Exercise 5:\n",
        "<b><div style=\"text-align: right\">[POINTS: 2]</div></b>\n",
        "\n",
        "Now that you have the complete blog post stored in the variable `text`, you need to apply some basic pre-processing to it.\n",
        "\n",
        "**Tasks:**\n",
        "\n",
        "1. Remove punctuation marks, symbols, and numbers from `article`. Store the cleaned article in the variable `cleaned_article`.\n",
        "\n",
        "2. Tokenize `article` into its constituent words and store it in the variable `tokens`.\n",
        "\n",
        "3. Lowercase each token in `tokens`.\n",
        "\n",
        "4. Check each token in `tokens` to be a stopword and remove it if it is a stopword. Note: List of English stopwords are stored in the list `eng_stopwords`.  \n",
        "\n",
        "5. Create an object, `stemmer` of the class `PorterStemmer`, and use it to stem each token in `tokens`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "id": "O_YryETnJePF",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "468d8ef6e8f875ff71c9e933f8165ee4",
          "grade": false,
          "grade_id": "cell-7bc3cf13392bd8b4",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": [
          "Ex-5-Task-1"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "984af18b-4845-4014-e58c-372615a29a91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "even covid19 crisi struck plung world recess peopl busi adopt ai take varieti task global workforc work home ai becom import continu affect shape way world recov past econom recess 14 compani abl increas sale adopt digit solut consum behavior surpris consum turn onlin behavior began long pandem consum oper nearli entir onlin inde choic lockdown today consum choos reduc contact mean get good servic especi true younger higherincom consum today digit world one order food take class work without leav home trend unlik disappear crisi autom repetit task peopl focu creativ task busi take advantag uniqu time capit onlin market consum willing partak set tone busi function postcovid19 market leverag ai alway provid competit advantag power autom repetit time consum task previous done human ai human better spend time demand dynam task channel creativ need boost suppli chain effici reduc need human interact exactli benefit adopt ai help busi recov post covid19 data help process scale al make possibl today suppli chain reach level complex requir machin process track inform leverag data help minim need unnecessari human human interact enhanc manufactur level data process especi help manufactur food industri manufactur data use predict mainten save time money increas valu boost qualiti control comput vision reduc error ai enabl factori oper 247 locat littl extra cost boost effici food industri ai also help save time food industri predict trend prefer plan deliveri rout help forecast demand strengthen relationship custom engag chatbot asset especi use covid19 lockdown peopl reli onlin way get food enhanc decis make insight fraud detect leverag ai data also help compani decis make capabl privat public organ compani may start reli entir ai make decis behalf human may leverag provid way compani trace decis make back human ai also gener insight base pattern trend process larg quantiti data also help boost secur detect anomal behavior fraud earli unpreced person custom engag loyalti ai help compani understand engag custom unpreced level ai make possibl open door compani understand custom behavior segment demograph purchas pattern make better recommend ultim foster increas custom loyalti type advanc data analyt machin learn allow compani person servic way unthink ai better understand custom allow compani brand market effect relev way success remot work compani alreadi power ai better time transit remot work environ creat opportun demand labor compani also better abl simul normal work environ platform alreadi ai enabl busi across globe may realiz advantag adopt ai increas remot workforc futur compani better pois continu remot work need maintain clear consist commun space time holist financi sector ai also enhanc financi sector greater secur onlin payment spend help us interact digit economi daili basi financi institut becom bank custom embrac holist solut servic help ensur transpar assur custom provid better healthcar servic relev field certainli embrac ai healthcar ai leverag multitud way pandem scan lung xray predict diagnosi evalu worsen symptom sure begin ai may help forecast futur epidem help patient selfdiagnos also enabl biometr process larg amount medic data ai help healthcar organ build compar statist epidemiolog model alreadi see hospit forgo ineffici procedur aid frontlin worker effect covid19 busi societi larg long last ultim contribut resili dynam way life power leverag data unpreced power way ai promis transform busi also help busi recov covid19 ai come rescu us heed call copyright 2023 fusemachin inc right reserv privaci polici eula copyright 2023 fusemachin inc right reserv\n"
          ]
        }
      ],
      "source": [
        "eng_stopwords = stopwords.words('english')\n",
        "cleaned_article = None\n",
        "tokens = None\n",
        "\n",
        "### Ex-5-Task-1\n",
        "### BEGIN SOLUTION\n",
        "# YOUR CODE HERE\n",
        "\n",
        "cleaned_article = re.sub('[^A-Za-z]+',' ',article)\n",
        "tokens = word_tokenize(cleaned_article)\n",
        "tokens = [token.lower() for token in tokens]\n",
        "tokens = [token for token in tokens if token not in eng_stopwords]\n",
        "stemmer = PorterStemmer()\n",
        "tokens = [stemmer.stem(token) for token in tokens]\n",
        "# raise NotImplementedError()\n",
        "### END SOLUTION\n",
        "\n",
        "# getting back the original text\n",
        "preprocessed_article = \" \".join(tokens)\n",
        "print(preprocessed_article)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "ZJv4VYKlK2Ob",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "216394273496a7fcfcd2debba72bcaa2",
          "grade": true,
          "grade_id": "cell-8034ba106e4277a1",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [
          "Ex-5-Task-1"
        ]
      },
      "outputs": [],
      "source": [
        "### INTENTIONALLY LEFT BLANK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnZnoZwGMPxl"
      },
      "source": [
        "## Exercise 6: Calculating Tfidf Scores\n",
        "<b><div style=\"text-align: right\">[POINTS: 2]</div></b>\n",
        "\n",
        "Now that you have preprocessed the article let's compute its TF-IDF representation. You will use the vocabulary, and the IDF values computed previously on the medium articles dataset. So you will not fit the `count_vectorizer` and `transformer` objects on the new article. Instead, you will use them to transform the new article into a TF-IDF vector.\n",
        "\n",
        "**Tasks:**\n",
        "\n",
        "1. Transform the `preprocesssed_article` into a count vector using the `transform` method of the `count_vectorizer` object you created earlier.\n",
        "\n",
        "2. Transform the count vector created by the `count_vectorizer` into a TF-IDF vector using the `transform` method of the `transformer` object you created earlier. Store the result in `article_tfidf`.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "id": "KARhaAgvLmie",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "4408ab8eb41aa79e01bcbe991d8c45a1",
          "grade": false,
          "grade_id": "cell-6a17a85afec95289",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": [
          "Ex-6-Task-1"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "bae71f3f-4e42-4b70-c1dc-546fb93e7d30"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   aaand  aaauu  aae  aah  aarhu  aaron   ab  aback  abaixo  abandon  ...  \\\n",
              "0    0.0    0.0  0.0  0.0    0.0    0.0  0.0    0.0     0.0      0.0  ...   \n",
              "\n",
              "   zone  zoo  zoologist  zoom  zorn  zoubin  ztm  zuckerberg  zumba  zurich  \n",
              "0   0.0  0.0        0.0   0.0   0.0     0.0  0.0         0.0    0.0     0.0  \n",
              "\n",
              "[1 rows x 11521 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f0630f7e-0ec7-4d5d-b757-64c6eaf56796\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>aaand</th>\n",
              "      <th>aaauu</th>\n",
              "      <th>aae</th>\n",
              "      <th>aah</th>\n",
              "      <th>aarhu</th>\n",
              "      <th>aaron</th>\n",
              "      <th>ab</th>\n",
              "      <th>aback</th>\n",
              "      <th>abaixo</th>\n",
              "      <th>abandon</th>\n",
              "      <th>...</th>\n",
              "      <th>zone</th>\n",
              "      <th>zoo</th>\n",
              "      <th>zoologist</th>\n",
              "      <th>zoom</th>\n",
              "      <th>zorn</th>\n",
              "      <th>zoubin</th>\n",
              "      <th>ztm</th>\n",
              "      <th>zuckerberg</th>\n",
              "      <th>zumba</th>\n",
              "      <th>zurich</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows × 11521 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f0630f7e-0ec7-4d5d-b757-64c6eaf56796')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f0630f7e-0ec7-4d5d-b757-64c6eaf56796 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f0630f7e-0ec7-4d5d-b757-64c6eaf56796');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "article_tfidf = None\n",
        "\n",
        "### Ex-6-Task-1\n",
        "### BEGIN SOLUTION\n",
        "# YOUR CODE HERE\n",
        "count_vector = count_vectorizer.transform([preprocessed_article])\n",
        "article_tfidf = tfidf_transformer.transform(count_vector)\n",
        "# raise NotImplementedError()\n",
        "### END SOLUTION\n",
        "\n",
        "if (type(article_tfidf) != np.ndarray):\n",
        "    article_tfidf = article_tfidf.toarray()\n",
        "\n",
        "article_tfidf = pd.DataFrame(article_tfidf, columns=count_vectorizer.get_feature_names_out())\n",
        "article_tfidf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "HNnyg71qNrwJ",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "370a29a5ef4e86950a071ac98179da88",
          "grade": true,
          "grade_id": "cell-3d482d0e48fe9169",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [
          "Ex-6-Task-1"
        ]
      },
      "outputs": [],
      "source": [
        "### INTENTIONALLY LEFT BLANK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kr3D4xbMQr-2"
      },
      "source": [
        "## Exercise 7: Extracting the Keywords\n",
        "<b><div style=\"text-align: right\">[POINTS: 1]</div></b>\n",
        "\n",
        "You have the TF-IDF representation of the article. You can now pick the top $n$ words with the highest TF-IDF values as the keywords.\n",
        "\n",
        "**Tasks:**\n",
        "\n",
        "1. Sort the columns of the dataframe `article_tfidf` in the descending order of their values. Store the sorted dataframe as `article_tfidf_sorted`.\n",
        "\n",
        "2. Pick the first $20$ column names from the sorted `article_tfidf` and store it in the variable `keywords`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "id": "dCfBt5lGOs6F",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "9bf3bb27095398d26616a8801d3720b3",
          "grade": false,
          "grade_id": "cell-7505c5aef18237c1",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": [
          "Ex-7-Task-1"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee5cbeff-e9f7-4702-a6c0-a2a0f4e7b03c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ai',\n",
              " 'custom',\n",
              " 'help',\n",
              " 'compani',\n",
              " 'busi',\n",
              " 'leverag',\n",
              " 'consum',\n",
              " 'food',\n",
              " 'remot',\n",
              " 'boost',\n",
              " 'adopt',\n",
              " 'unpreced',\n",
              " 'recov',\n",
              " 'healthcar',\n",
              " 'recess',\n",
              " 'crisi',\n",
              " 'manufactur',\n",
              " 'onlin',\n",
              " 'loyalti',\n",
              " 'holist']"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "article_tfidf_sorted = None\n",
        "\n",
        "### Ex-7-Task-1\n",
        "### BEGIN SOLUTION\n",
        "# YOUR CODE HERE\n",
        "article_tfidf_sorted = article_tfidf.sort_values(by=0, axis=1, ascending=False)\n",
        "keywords = article_tfidf_sorted.columns[:20]\n",
        "# raise NotImplementedError()\n",
        "### END SOLUTION\n",
        "\n",
        "keywords = list(keywords)\n",
        "keywords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "hqWrgAhes5tq",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "29fd617a15264f8860dafdd0f1ac8729",
          "grade": true,
          "grade_id": "cell-f949d3028db7fd1f",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [
          "Ex-7-Task-1"
        ]
      },
      "outputs": [],
      "source": [
        "### INTENTIONALLY LEFT BLANK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRQBMXRhO5zI"
      },
      "source": [
        "You can see that the extracted keywords give us a pretty good idea about what the article is about."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNj4N33ewU1B"
      },
      "source": [
        "# Well Done!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}