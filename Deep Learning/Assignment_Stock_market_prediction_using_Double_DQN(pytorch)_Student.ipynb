{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "RDI-1Bcs5uex",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9fdb1c7bc4cbcaf50148677eb931153b",
     "grade": false,
     "grade_id": "cell-2fae3d6cdc938d07",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Assignment : Stock market prediction with Double DQN\n",
    "---\n",
    "\n",
    "<b><div style=\"text-align: right\">[TOTAL POINTS: 15]</div></b>\n",
    "\n",
    "In this assignment, we will solve stock market prediction problem using Double DQN. By the end of the Assignment you will be able to:\n",
    "\n",
    "- implement Double DQN algorithm in stock market prediction problem \n",
    "\n",
    "Before proceeding to the exercises, let's first know about the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "hkMNZWTRjo62",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e1bc8b7b8d0b3457de3dd3e2e543dee9",
     "grade": false,
     "grade_id": "cell-7d032aabd4245860",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Environment\n",
    "\n",
    "`AnyTrading` is the trading environment provided by OpenAI gym environment. Generally, this environment provides two markets to implement: `Forex` and `Stock`. The main goal of this environment is to provide different trading markets for testing different RL algorithms in the same way as other gym environments.\n",
    "\n",
    "We will be using `Stock` trading environment for this implementation. Let's understand more about it.\n",
    "\n",
    "### Properties\n",
    "\n",
    "> - `action_space:` There are two possible actions: \n",
    "    - `Sell: 0`\n",
    "    - `Buy: 1`\n",
    "- `observation_space:` It returns the state of the environment with size equals to `window_size x 2`. Here `window_size` determines how large or small state we are looking for and the later `2` dimensions value represents the following informations:\n",
    "  - `price:` The zeroth element of the each window's array is price value.\n",
    "  - `price_diff:` The oneth element of the each window's array is price difference value.\n",
    "- `shape:` It gives shape of a single observation. Generally, it is `window_size x 2`.\n",
    "- `history:` It stores all the info of all steps.\n",
    "- `frame_bound:` It is tuple that represents lower index and upper index of the environments' dataframe `df` to be used for training.\n",
    "- `window_size:` It is a integer value that determines the size of our state space. For eg., if `window_size=10`, our state space size will be of `10x2`. Here, `2` represents `price` and `price_diff` quantities.\n",
    "\n",
    "### Methods\n",
    "\n",
    "> - `seed:` It is same as typical gym `env.seed()` method.\n",
    "- `reset:` It is same as typical gym `env.reset()` method.\n",
    "- `step:` It is same as typical gym `env.step()` method.\n",
    "- `render:` It is same as typical gym `env.render()` method.\n",
    "- `render_all:` Unlike gym, it renders the whole environment.\n",
    "- `close:` It is same as typical gym `env.close()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "xf6ywZ2KWIHB",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a1354286604cc83f325b78f610010792",
     "grade": false,
     "grade_id": "cell-9b420c537bc879ec",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Assignment overview\n",
    "\n",
    "This assignment is divided into two major exercises. Due to free mode of this assignment, there are no explict tasks inside the exercise. However, You will perform various tasks inside these two exercises. \n",
    "\n",
    "You can implement Double DQN algorithm freely as you want with certain restrictions. These restrictions will be explained inside each exercise. \n",
    "\n",
    "These are some overview of tasks that you will complete in each exercise.\n",
    "\n",
    "### Exercise 1: Create an agent class\n",
    "- Initialize agent class\n",
    "- Implement methods for Double DQN algorithms including build network, experience replay buffer, run episodes, etc. \n",
    "\n",
    "### Exercise 2: Train the model\n",
    "- Create an stock market environment\n",
    "- Train the model and return reward history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "x9V7mhrUX6RR",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c904d4276c9e2aec3c8deab5547c30b9",
     "grade": false,
     "grade_id": "cell-2883002f422177f6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Import libraries\n",
    "Let's first import all the necessary libraries for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 4983,
     "status": "ok",
     "timestamp": 1646113576683,
     "user": {
      "displayName": "Aaditya Chapagain",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiT9mQguxG1ZXHw6f3xf2M7DoCasHMi-oAASsEK=s64",
      "userId": "11150217817411318700"
     },
     "user_tz": -345
    },
    "id": "kw4WBNCrAuVS",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8dc76c56667a739aedf550faedb37eca",
     "grade": false,
     "grade_id": "cell-127ccd113c32c61e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "44bbcf88-10f6-4e28-9bbc-6595031572c8"
   },
   "outputs": [],
   "source": [
    "!pip install gym_anytrading # for gym trading environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "84estmgZAwMw"
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# for stock market environment\n",
    "import gym\n",
    "import gym_anytrading\n",
    "from gym_anytrading.envs import TradingEnv, ForexEnv, StocksEnv, Actions, Positions \n",
    "from gym_anytrading.datasets import FOREX_EURUSD_1H_ASK, STOCKS_GOOGL\n",
    "\n",
    "# for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# you can import additional libraries here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "rPlnPFjAxa2t",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ddb420ac0c4f4be1a64e5b33c180d8c1",
     "grade": false,
     "grade_id": "cell-b30f306e38c9cc21",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "These are some essential libraries. If you need to import other libraries you can import in above section the way you want to implement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "hBp44Uq-uKZj",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e6d56fcdced6f2250decd485aa63744c",
     "grade": false,
     "grade_id": "cell-5984943076f879dd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Agent environment\n",
    "\n",
    "This section consists of overall agent environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "UAM-Yow4Yg94",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d711b00daa964e7356ea740cc36a4aec",
     "grade": false,
     "grade_id": "cell-11853a963fc240bc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 1: Create an agent class\n",
    "\n",
    "<b><div style=\"text-align: right\">[Marks: 5]</div></b>\n",
    "\n",
    "In this exercise, you will implement all methods for Double DQN algorithm inside the agent class from scratch. You can refer to the assignment or any other material to implement the agent.\n",
    "\n",
    "There are few things you need to consider before writing your tasks:\n",
    "\n",
    "- The agent class should be `class Agent` and it should take initial arguments as: `state_size`, `num_actions` and `window_size`.\n",
    "  - `state_size:` It is the size of the stock environment's observation space (i.e., `env.observation_space.shape`)\n",
    "  - `num_actions:` It the number of possible actions of the environment (i.e., `env.action_space.n`).\n",
    "  - `window_size:` It determines how large state space you want. \n",
    "\n",
    "- Implement `build_network()` method to build architecture for target Q network and current Q network. This method can take `state_size` and `num_actions` as an input variables and it should return the `model`. You can freely create your own architecture for this environment but remember total number of parameter of the both model should not exceed `8000` i.e., `number of network parameters < 8000`.\n",
    "\n",
    "- Implement `train()` method that takes an environment object and number of epochs as arguments. It should return `all episodic rewards in list` during the training.\n",
    "\n",
    "Possible hyperparameters:\n",
    "\n",
    "- `discount rate (`$\\gamma$`)= 0.9`\n",
    "- `learning rate (lr) = 0.0005`\n",
    "- `batch size = 32`\n",
    "- `buffer memory length = 50000`\n",
    "- `update target step = 200`\n",
    "\n",
    "Besides these requirements and possible hints, you are free to implement Double DQN algorithm, especially the one you studied in this unit. Furthermore, it would be best if you considered varying the model architecture, exploration rate and different hyperparameters to get best result.\n",
    "\n",
    "For your reference, the following is the major difference in target equation in Double DQN algorithm compared to DQN algorithm. For complete algorithm you can refer to DQN chapter.\n",
    "\n",
    "> - if episode terminates at t+1: \n",
    ">     - $y_j = r_j$\n",
    "> - else \n",
    ">     - $y_j = r_j+ \\gamma\\hat{Q}(\\phi_{j+1}, argmax_{a'}\\hat{Q}(\\phi_{j+1}, a'; \\theta);\\theta^-)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution Code Snippet\n",
    "<details>\n",
    "    <summary style=\"color:red\">Click here and copy the code to cell below</summary>\n",
    "    \n",
    "        def __init__(self, state_size, num_actions=2, window_size=20):\n",
    "        self.num_actions = None\n",
    "        self.state_size = None\n",
    "        self.window_size = None\n",
    "        \n",
    "        self.lr = None\n",
    "        self.gamma = None\n",
    "        self.batch_size = None\n",
    "        self.epsilon = None\n",
    "        self.epsilon_min = None\n",
    "        self.epsilon_decay = None\n",
    "        self.rs = np.random.RandomState(seed=42)\n",
    "\n",
    "        self.memory = deque(maxlen=50000)\n",
    "        self.update_target = 200\n",
    "\n",
    "        self.q_network = None\n",
    "        self.target_network = None\n",
    "        self.optimizer = None\n",
    "        self.loss_fn=  None\n",
    "    \n",
    "    def build_network(self, state_size, num_actions=2):\n",
    "        size = state_size[0] * state_size[1]\n",
    "        model = nn.Sequential(\n",
    "          nn.Flatten(),\n",
    "          nn.Linear(None, None),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(None, None),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(None, None),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(None, None)\n",
    "        )\n",
    "        return model\n",
    "  \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self):\n",
    "        replay = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, done = map( list , zip(*replay))\n",
    "        states = torch.stack(states).reshape(-1, *self.state_size)\n",
    "        actions = torch.Tensor(actions).to(torch.int64)\n",
    "        rewards = torch.Tensor(rewards)\n",
    "        next_states = torch.stack(next_states).reshape(-1, *self.state_size)\n",
    "        done = torch.Tensor(done)\n",
    "        return states, actions, rewards, next_states, done\n",
    "\n",
    "    def optimize_network(self, states, actions, rewards, next_states, dones):\n",
    "        self.target_network.train(False)\n",
    "        self.q_network.train(False)\n",
    "        with torch.no_grad():\n",
    "            max_actions = None\n",
    "            update_values = None\n",
    "            targets = None\n",
    "\n",
    "            actions_one_hot = None\n",
    "\n",
    "        self.q_network.train(True)\n",
    "        q_values = self.q_network(states)\n",
    "        action_values = None\n",
    "        loss = None\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        self.optimizer.step()\n",
    "    \n",
    "    def run_episode(self, env, frame_count):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        state = torch.Tensor(state).unsqueeze(0)\n",
    "        while not done:\n",
    "            frame_count += 1\n",
    "            if self.epsilon > self.rs.rand():\n",
    "                action = None\n",
    "            else:\n",
    "                q_values = None\n",
    "                action = None\n",
    "            if self.epsilon > self.epsilon_min:\n",
    "                self.epsilon -= self.epsilon_decay\n",
    "\n",
    "            next_obs, reward, done, _ = env.step(action)\n",
    "            next_obs = torch.Tensor(next_obs).unsqueeze(0)\n",
    "            episode_reward += None\n",
    "\n",
    "            self.remember(state, action, reward, next_obs, done)\n",
    "\n",
    "            state = next_obs\n",
    "            \n",
    "            if frame_count > self.batch_size:\n",
    "                sample_batch = self.sample()\n",
    "                self.optimize_network(*sample_batch)\n",
    "\n",
    "            if (frame_count + 1) % self.update_target == 0:\n",
    "                self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "        return episode_reward, frame_count\n",
    "    \n",
    "    def train(self, env, num_epochs=50):\n",
    "        frame_count = 0\n",
    "        reward_history = []\n",
    "        for epoch in range(None):\n",
    "            episodic_reward, frame_count = None\n",
    "            reward_history.append(None)\n",
    "            print('Episode {} || Rewards:{} || Avg reward:{}'.format(epoch, round(episodic_reward, 2), np.average(reward_history)))\n",
    "        plt.cla()\n",
    "        env.render_all()\n",
    "        plt.show()\n",
    "        return reward_history\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "VxqcirEuAE8A",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ebf68ef03d9de794f9ecb7751b259de8",
     "grade": false,
     "grade_id": "cell-7a0b04e0f17210af",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "Ex-1-Task-1"
    ]
   },
   "outputs": [],
   "source": [
    "### Ex-1-Task-1\n",
    "class Agent(object):\n",
    "    '''An agent class'''\n",
    "    ## TODO:\n",
    "    # Write all methods required for agent class\n",
    "    # Must have build_network() method and train() method at least as described\n",
    "    # in above section\n",
    "    ### BEGIN SOLUTION\n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "m6jyiUQe9Y6k",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0fceb3ec3bf1337085de91d719b187bb",
     "grade": true,
     "grade_id": "cell-d1a7e202296b1be9",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": [
     "Ex-1-Task-1"
    ]
   },
   "outputs": [],
   "source": [
    "# Intentionally left blank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "hEyp-aKEqdYT",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3adbd116c8234f2b35a5ec5a6e690abd",
     "grade": false,
     "grade_id": "cell-df6c27881537d5a9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 2: Train the model\n",
    "<b><div style=\"text-align: right\">[Marks: 10]</div></b>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "xIYGwT-oAI0K",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0ad430ad74ce0eecdd0b191ec82a0779",
     "grade": false,
     "grade_id": "cell-7a7d944df2bd8f0a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In this exercise, you will create a stock environment and train your agent.\n",
    "\n",
    "These are the guidlines for this exercise:\n",
    "- First create a stock environment by `gym.make(env_name, frame_bound, window_size)`.\n",
    "  - `env_name = stocks-v0`\n",
    "  - `frame_bound = (800, 1000)`\n",
    "  - `window_size = 15` \n",
    "- Initialize an agent with following parameters:\n",
    "  - `state_size`\n",
    "  - `num_actions`\n",
    "  - `window_size`\n",
    "- Train agent upto `20 epochs` and store all the rewards into `reward_history` variable. \n",
    "\n",
    "> #### **You will get full marks if the average rewards for 20 epochs training is greater than 50.00**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 641
    },
    "deletable": false,
    "executionInfo": {
     "elapsed": 8390,
     "status": "ok",
     "timestamp": 1646119056831,
     "user": {
      "displayName": "Aaditya Chapagain",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiT9mQguxG1ZXHw6f3xf2M7DoCasHMi-oAASsEK=s64",
      "userId": "11150217817411318700"
     },
     "user_tz": -345
    },
    "id": "wC63QGtpq5Vl",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "37b82083aa86a24690647b59d61c4315",
     "grade": false,
     "grade_id": "cell-5ff53adb0de5c299",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "18581e68-88db-4583-e397-a874dc151a24",
    "tags": [
     "Ex-2-Task-1"
    ]
   },
   "outputs": [],
   "source": [
    "### Ex-2-Task-1\n",
    "frame_bound = (800, 1000)\n",
    "window_size = 15\n",
    "num_epochs = 20\n",
    "env = None  #implement gym.make()\n",
    "## use seed value of 42 in env.seed()\n",
    "\n",
    "state_size = None\n",
    "num_actions = None\n",
    "\n",
    "agent = None #initialize Agent(state_size, num_actions, window_size)\n",
    "\n",
    "\n",
    "reward_history = None # store history using agent.train()\n",
    "### BEGIN SOLUTION\n",
    "# your code here\n",
    "raise NotImplementedError\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 641
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 8789,
     "status": "ok",
     "timestamp": 1646119088314,
     "user": {
      "displayName": "Aaditya Chapagain",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiT9mQguxG1ZXHw6f3xf2M7DoCasHMi-oAASsEK=s64",
      "userId": "11150217817411318700"
     },
     "user_tz": -345
    },
    "id": "-juEiPWsAIMa",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "af3916a768692e29334e3c9ccfa686d7",
     "grade": true,
     "grade_id": "cell-2b9b7d911c36d06e",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "9d21e29f-de48-4165-de00-c06224878971",
    "tags": [
     "Ex-2-Task-1"
    ]
   },
   "outputs": [],
   "source": [
    "assert env is not None\n",
    "assert frame_bound is not None\n",
    "assert window_size is not None\n",
    "assert state_size is not None\n",
    "assert num_actions is not None\n",
    "assert agent is not None\n",
    "assert num_epochs is not None\n",
    "assert reward_history is not None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "Z9dOTZ6gWsTR",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8c0bcf2c8524d042f1999015c60fca91",
     "grade": false,
     "grade_id": "cell-8feca12a5a1380fc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "Congrats!! You have successfully implemented Double DQN algorithm for stock market prediction. By now, you may have fully understand the Double DQN algorithm. You can also try Dueling DQN algorithm in various other openAI gym environment as well."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment_Stock_market_prediction_using_Double_DQN(pytorch).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
