{"cells":[{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"LRzSSXMDeDMP","nbgrader":{"cell_type":"markdown","checksum":"8660127d8d0002cf7062c764f1128f64","grade":false,"grade_id":"cell-244e596481a3b295","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["# Image Reconstruction using Variational Autoencoders (VAE)\n","\n","> Indented block\n","\n","\n","\n","<b> <div style=\"text-align: right\">[TOTAL POINTS: 10]</div></b>\n","## Learning Objective\n","In this assignment, we will be implementing VAE to reconstruct the Fashion MNIST images. \n","\n","1. Create the encoder model.\n","2. Create the decoder model.\n","3. Implement KL-Divergence Loss on VAE Model.\n","4. Train the VAE model.\n","5. Visualize original image, latent space and reconstructed image.\n"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"_WiJyJ3SePf1","nbgrader":{"cell_type":"markdown","checksum":"2fe39c6ea2e7eea1e84a7d9f28f41a48","grade":false,"grade_id":"cell-5f334664f5c5b2d8","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["## Dataset Description\n","\n","### Fashion MNIST Dataset\n","\n","*Source* : https://github.com/zalandoresearch/fashion-mnist/blob/master/LICENSE<br>\n","*Author* : Zalando SE<br>\n","*License* : Copyright Â© 2017 Zalando SE, https://tech.zalando.com<br>\n","\n","This dataset consists of 60000 28x28 grayscale images of 10 categories of fashion clothings. The categories are 'T-Shirt/Top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag' and 'Ankle Boot'. Likewise, there are 10000 test images. \n","\n","Dataset   $\\hspace{20mm}$   No of images<br>\n","Train Set$\\hspace{25mm}$ 60000<br>\n","Test Set  $\\hspace{25mm}$ 20000<br>\n"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"j6NtZbb1ePP-","nbgrader":{"cell_type":"markdown","checksum":"00301ea7fa9c4f37325cad26b020f8bd","grade":false,"grade_id":"cell-4e8f75fb7b4e1e64","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["## Import required libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"KaIGAJDbeXJF","nbgrader":{"cell_type":"code","checksum":"4ab460be77c6569e7a3fdedb3d4127b9","grade":false,"grade_id":"cell-347b4518ef389eb2","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["### READ ONLY \n","import tqdm\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import torch\n","from torch import nn\n","import torchvision\n","\n","np.random.seed(42)\n","torch.manual_seed(42)\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')\n","device"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"99BqLpmAeVvN","nbgrader":{"cell_type":"markdown","checksum":"670960a703abe129d8fd03ab5c1f280f","grade":false,"grade_id":"cell-737d3efe7b573cb2","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["## Load Dataset\n","\n","### Dataset Preparation Steps\n","\n","1. The Fashion MNIST dataset is loaded from `torchvision.datasets.MNIST`\n","2. The dataset contains pixel that take value from 0 to 1. So we don't have to perform normalization by dividing the train and test images by 255.0. "]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"jdJPhTtFhr1V","nbgrader":{"cell_type":"code","checksum":"139a10e810a78a9334c15fb0ee8d306d","grade":false,"grade_id":"cell-b25b20b545e85e12","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["### READ ONLY\n","transform = torchvision.transforms.ToTensor()\n","train_dataset = torchvision.datasets.FashionMNIST('./data', train=True, download=True,\n","                                           transform=transform)\n","test_dataset = torchvision.datasets.FashionMNIST('./data', train=False, download=True,\n","                                          transform=transform)\n","print(\"Examples in train and test set:\", len(train_dataset), len(test_dataset))\n","image_shape = train_dataset[0][0].shape\n","print(\"Image shape(channels, height, width):\", image_shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"SRq81noPqEs3","nbgrader":{"cell_type":"code","checksum":"3ccc1ed02d74e4d78c4538f3a78eea88","grade":false,"grade_id":"cell-4bdb76fc2f34cbfc","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["### READ ONLY\n","batch_size = 128 #Batch_size per iteration\n","\n","trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n","                                          shuffle=True, num_workers=2)\n","testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n","                                         shuffle=False, num_workers=2)\n","len(trainloader), len(testloader)"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"GG5e4coUhzQV","nbgrader":{"cell_type":"code","checksum":"5440f495e751b2cfd449ad7b1ac74796","grade":false,"grade_id":"cell-a792b83e57ff9f8e","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["### READ ONLY\n","\n","class Sampling(nn.Module):\n","  \"\"\"\n","  Class for sampling latent vector(z), the vector encoding of the digits using z_mean and z_log_var\n","  \n","  Methods\n","  --------\n","  forward(z_mean, z_log_var)\n","      computes the deterministic latent space variable\n","      \n","    \"\"\"\n","\n","\n","  def forward(self, z_mean, z_log_var):\n","    \"\"\"\n","    computes the deterministic latent space variable\n","    \n","    Parameters\n","    ---------\n","    z_mean: Tensor\n","        mean values of the approximate posterior probability\n","        \n","    z_log_var: Tensor\n","        log_scale values of the variance of the approximate posterior probability\n","        \n","    Returns\n","    -------\n","    z:Tensor\n","        deterministic latent space variable\n","    \"\"\"\n","    stddev = torch.exp(0.5*z_log_var)\n","    epsilon = torch.randn_like(stddev)\n","    return z_mean + stddev*epsilon"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"piS6pvtliCbW","nbgrader":{"cell_type":"markdown","checksum":"31a748bc927a797c547930e474402b9e","grade":false,"grade_id":"cell-0d4ecd3473c30690","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["## Exercise 1: Create Encoder Model\n","\n","**[Points: 4]**\n","\n","**Task** : Implement Encoder Model such that it returns mean, log-variance and latent space.\n","\n","**Steps** : \n","1. Inside sequential module add following layers in order\n","    * 2D convolution with `in_channels=image_shape[0]`, `out_channels=32`, `5 x 5 kernels`, `stride of 1`, `padding=2`\n","    * ReLU activation\n","    * 2D convolution with `in_channels=32`, `out_channels=32`, `4 x 4 kernels`, `stride of 2`, `padding=1`\n","    * ReLU activation\n","    * 2D convolution with `in_channels=32`, `out_channels=64`, `4 x 4 kernels`, `stride of 2`, `padding=1`\n","    * ReLU activation\n","    * Flatten the output\n","    * Linear layer with `in_features=prod(out_channels of last conv layer; 64, image height//4, image width//4)`\n","    * ReLU activation\n","2. Implement forward method with following operation in order\n","    * Call sequential layer created in step 1 passing inputs\n","    * Pass the output to mean and log_var layer and store output in variables `z_mean` and `z_log_var` respectively\n","    * Call sampling layer and store result in variable `z`"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"eb1a78f815b664abfff8ed3dd3be4a80","grade":false,"grade_id":"cell-ffa5208a156e4be3","locked":true,"schema_version":3,"solution":false,"task":false},"id":"N81LIp7hBubs"},"source":["### Solution Code Snippet\n","<details>\n","    <summary style=\"color:red\">Click here and copy the code to cell below</summary>\n","    \n","        self.enc_layers = nn.Sequential(\n","          \n","            nn.Conv2d(None, None, kernel_size=None, stride=None, padding=None), # out_shape = image_shape\n","            nn.ReLU(),\n","            nn.Conv2d(None, None, kernel_size=None, stride=None, padding=None), # out_shape = image_shape//2\n","            nn.ReLU(),\n","            nn.Conv2d(None, None, kernel_size=None, stride=None, padding=None), # out_shape = image_shape//4\n","            nn.ReLU(),\n","            nn.Flatten(),\n","            nn.Linear(None, None),\n","            nn.ReLU(),\n","          \n","        )\n","\n","        self.dense_mean = nn.Linear(196, latent_dim)\n","        self.dense_log_var = nn.Linear(196, latent_dim)\n","        self.sampling = Sampling()\n","    \n","    def forward(self, inputs):\n","        \n","        \"\"\"\n","        Forward pass for encoder network\n","        \n","        Parameters\n","        ----------\n","        inputs: Tensor\n","                Input data to the encoder network\n","        \n","        Returns\n","        -------\n","        z_mean: Tensors \n","            mean values of the approximate posterior probability\n","        z_log_var: Tensors \n","            log_scale values of the variance of the approximate posterior probability\n","        z: Tensors\n","            deterministic latent space variables, output of the encoder network\n","        \"\"\"        \n","        z_mean, z_log_var, z = None, None, None\n","        \n","        h = None                #Type your code here\n","        z_mean = None           #Type your code here\n","        z_log_var = None        #Type your code here\n","        z = None                #Type your code here\n","\n","</details>"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"id":"RDXqlSEQh605","nbgrader":{"cell_type":"code","checksum":"dd1a5a665a2b10d1a790fe7b04dbe37f","grade":false,"grade_id":"cell-91821581215d39b0","locked":false,"schema_version":3,"solution":true,"task":false},"tags":["Ex-1-Task-1"]},"outputs":[],"source":["### Ex-1-Task-1\n","\n","class Encoder(nn.Module):\n","    \"\"\"Generates (z_mean, z_log_var, z) from Fashion MNIST\"\"\"\n","\n","    \"\"\"\n","    Encoder of VAE network\n","    \n","    Properties\n","    ---------- \n","    latent_dim: int\n","            size of latent dim vector in which image will be encoded\n","    image_shape: Tuple or torch.Size\n","            size of an image in the format: (Channels, Height, Weight)\n","            \n","    \n","    inputs: Tensor or numpy.ndarray\n","            input data to the encoder network\n","        \n","    Methods\n","    -------\n","    forward(inputs)\n","        Forward pass for encoder network\n","    \"\"\"\n","     \n","    def __init__(self,\n","                 image_shape,\n","                 latent_dim,\n","                ):\n","        super(Encoder, self).__init__()\n","\n","        self.enc_layers = nn.Sequential(\n","            \n","            nn.Conv2d(image_shape[0], 32, kernel_size=5, stride=1, padding=2), # out_shape = image_shape\n","            nn.ReLU(),\n","            nn.Conv2d(32, 32, kernel_size=4, stride=2, padding=1), # out_shape = image_shape//2\n","            nn.ReLU(),\n","            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1), # out_shape = image_shape//4\n","            nn.ReLU(),\n","            nn.Flatten(),\n","            nn.Linear(64*(image_shape[1]//4)*(image_shape[2]//4), 196),\n","            nn.ReLU(),\n","            \n","        )\n","\n","        self.dense_mean = nn.Linear(196, latent_dim)\n","        self.dense_log_var = nn.Linear(196, latent_dim)\n","        self.sampling = Sampling()\n","    \n","    def forward(self, inputs):\n","        \n","        \"\"\"\n","        Forward pass for encoder network\n","        \n","        Parameters\n","        ----------\n","        inputs: Tensor\n","                Input data to the encoder network\n","        \n","        Returns\n","        -------\n","        z_mean: Tensors \n","            mean values of the approximate posterior probability\n","        z_log_var: Tensors \n","            log_scale values of the variance of the approximate posterior probability\n","        z: Tensors\n","            deterministic latent space variables, output of the encoder network\n","        \"\"\"        \n","        z_mean, z_log_var, z = None, None, None\n","        ### BEGIN SOLUTION\n","        # your code here\n","        raise NotImplementedError\n","        ### END SOLUTION\n","\n","        return z_mean, z_log_var, z"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"YC8VLHtqj871","nbgrader":{"cell_type":"code","checksum":"abe99381aa7bb51b75f97b6f6ad83faa","grade":false,"grade_id":"cell-f3799a01bcc67a8b","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["### READ ONLY\n","latent_dim = 16\n","encoder_ = Encoder(image_shape, latent_dim).to(device)\n","encoder_"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"_0PJA8t9kEWY","nbgrader":{"cell_type":"code","checksum":"d940a48c4d9cc5512fbaeae1e2c5381d","grade":true,"grade_id":"cell-f6075d1a62d05fc4","locked":true,"points":4,"schema_version":3,"solution":false,"task":false},"tags":["Ex-1-Task-1"]},"outputs":[],"source":["### Intentionally left blank"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"H3Kv86B0WIz4","nbgrader":{"cell_type":"markdown","checksum":"dd255ca5404715ca79b55a5e6d2f001b","grade":false,"grade_id":"cell-2028195764de0e7c","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["## Exercise 2: Create Decoder Model\n","\n","**[Points: 4]**\n","\n","Implement Decoder Model according to following details.\n","\n","**Task**\n","1. Initialise a Linear layer with in_features `latent_dim` and out_features `64*(image_height//4)*(image_width//4)`.\n","2. Relu Activation\n","3. Reshape the previous layer into `(64, image_height//4, image_width//4)`. Use the `Reshape` Module defined in the cell below.\n","4. Implement 2D convolution tranpose(`nn.ConvTranspose2d`) with `(in_channels, out_channels, kernel, stride, padding)=(64, 32, 4, 2, 1)`\n","5. Relu Activation\n","6. Implement 2D convolution tranpose with `(in_channels, out_channels, kernel, stride, padding)=(32, 32, 4, 2, 1)`\n","7. Relu Activation\n","8. Implement 2D convolution tranpose with `(in_channels, out_channels, kernel, stride, padding)=(32, image_shape[0], 5, 1, 2)`\n","9. Sigmoid Activation\n","10. Implement `forward` method passing the inputs to the Sequential module implemented with above layers."]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"id":"VIopEWkBWJ09","nbgrader":{"cell_type":"code","checksum":"9239a2e781d7a4450e929c73d16e3e28","grade":false,"grade_id":"cell-6b4ab20e6020de96","locked":false,"schema_version":3,"solution":true,"task":false},"tags":["Ex-2-Task-1"]},"outputs":[],"source":["### Ex-2-Task-1\n","\n","class Reshape(nn.Module):\n","    \"\"\"\n","    Reshape the image to target shape, ignores the batching dim=0\n","    \"\"\"\n","    def __init__(self, target_shape):\n","        super(Reshape, self).__init__()\n","        self.target_shape = target_shape\n","    \n","    def forward(self, inputs):\n","        return inputs.view((-1, *self.target_shape))\n","\n","class Decoder(nn.Module):\n","    \"\"\"\n","    Reconstructs Fashion MNIST from latent variable obtained from Encoder\n","\n","    Decoder of VAE network\n","    \n","    Properties\n","    ----------    \n","    latent_dim : int\n","        No.of nodes of the latent space representation\n","    dense_dim  : int\n","        The product of the shape of the output obtained from last convolution layer of Encoder Model.\n","        The decoder converts latent space to that dense shape\n","    reshape_dim: \n","        The shape of the output obtained from last convolution layer of Encoder Model.\n","        The decoder reshapes the previous shape to reshape_dim\n","\n","        \n","    Methods\n","    -------\n","    forward(inputs)\n","        Forward pass for decoder network\n","        \n","    \"\"\"\n","    def __init__(self,\n","                 image_shape,\n","                 latent_dim,\n","                 ):\n","        super(Decoder, self).__init__()\n","        \n","        \"\"\"\n","        Task: Initialize the variables as stated above.\n","        \n","        \"\"\"\n","\n","        self.dec_layers = nn.Sequential(\n","            ### BEGIN SOLUTION\n","            # your code here\n","            raise NotImplementedError\n","            ### END SOLUTION\n","        )\n","        \n","    def forward(self, inputs):\n","        \n","        \"\"\"\n","        Forward pass for decoder network\n","        \n","        Parameters\n","        ----------\n","        \n","        inputs: Tensor\n","            input data(latent space features) to the decoder network.\n","            \n","        Returns\n","        -------\n","        x : Tensor\n","            output of the decoder network. Reconstructed output.\n","        \"\"\"\n","        \n","\n","        x = self.dec_layers(inputs)\n","\n","\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"DHDyEPL3WNbp","nbgrader":{"cell_type":"code","checksum":"3a8482007d273612b67da36869217257","grade":false,"grade_id":"cell-c3559d1c0c7d52bb","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["### READ ONLY\n","\n","decoder_ = Decoder(image_shape=image_shape,\n","                   latent_dim=latent_dim).to(device)\n","decoder_"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"BN5LfSO_WPyC","nbgrader":{"cell_type":"code","checksum":"054f4eb8ca8544d31d03cd8253b60cdc","grade":true,"grade_id":"cell-e90960b34368048f","locked":true,"points":4,"schema_version":3,"solution":false,"task":false},"tags":["Ex-2-Task-1"]},"outputs":[],"source":["### Intentionally left blank\n"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"DST_L8VUlJ_C","nbgrader":{"cell_type":"markdown","checksum":"bc24665106505fac1ce97ec4cbce5db1","grade":false,"grade_id":"cell-81ad458caad28754","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["## Create VAE Model "]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"u6GNyNapWfw5","nbgrader":{"cell_type":"code","checksum":"1cf6d12563773a91ca6409ba8e18a071","grade":false,"grade_id":"cell-d8a464be4dd63708","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["### READ ONLY\n","class VAE(nn.Module):\n","    \"\"\"\n","    Takes encoder and decoder model, and perform end-to-end image reconstruction\n","    \"\"\"\n","    def __init__(self, encoder, decoder, **kwargs):\n","        super(VAE, self).__init__(**kwargs)\n","        self.encoder = encoder\n","        self.decoder = decoder\n","\n","    def forward(self, inputs):\n","        reconstructed, z_mean, z_log_var = None, None, None\n","\n","        z_mean, z_log_var, z = self.encoder(inputs)\n","        reconstructed = self.decoder(z)\n","\n","        return reconstructed, z_mean, z_log_var"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"oBwQZi4Klnyn","nbgrader":{"cell_type":"code","checksum":"8eb430630254cce1354c68dfabe1c736","grade":false,"grade_id":"cell-18a662e16dabeb60","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["### READ ONLY\n","vae_ = VAE(encoder_, decoder_).to(device)\n","vae_"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"NntTSbTulnje","nbgrader":{"cell_type":"code","checksum":"9635f725be3c078ee19166c62ffaa09f","grade":false,"grade_id":"cell-8d2acdd142d9795a","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["# READ ONLY\n","\n","test_inp = torch.randn((10,*image_shape), device=device)\n","test_reconstructed, test_z_mean, test_z_log_var=vae_(test_inp)\n","\n","assert test_reconstructed.shape == (10, *image_shape), \"Invalid mean shape\"\n","assert test_z_mean.shape == (10,latent_dim), \"Invalid mean shape\"\n","assert test_z_log_var.shape == (10,latent_dim), \"Invalid log-variance shape\"\n"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"8PDZuLqYWd1O","nbgrader":{"cell_type":"markdown","checksum":"61804a1ae60a7e8bc425768fac81acf5","grade":false,"grade_id":"cell-33ca7675a0ce1aa5","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["## Exercise 3: KL Divergence Loss\n","\n","**[Points: 2]**\n","\n","**Task** : Implement KL-Divergence Loss\n","\n","### KL Divergence loss formula to be implemented.\n","\n","$$\\text{KL}(N(\\mu,\\sigma))=-\\frac{1}{2}(1+ln(\\sigma^2)-\\sigma^2-\\mu^2)$$\n","\n","**Aside from above formula, the loss should be averaged along the batch dimension. Also note that we pass log(variance) to the function below. Adjust the formula above accordingly.**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"id":"hnhsv_gNm1tZ","nbgrader":{"cell_type":"code","checksum":"4c8ec5f7741d678f5fb02ce3fbaeedb0","grade":false,"grade_id":"cell-33c7149507c99084","locked":false,"schema_version":3,"solution":true,"task":false},"tags":["Ex-3-Task-1"]},"outputs":[],"source":["### Ex-3-Task-1\n","\n","def KLDivLoss(mean, log_var):\n","    \"\"\"\n","    Implement KL div loss for Normal distribution with Standard Normal Distribution as reference probability distirbution\n","\n","    Params\n","    ----------------------\n","    mean: Tensor\n","        mean tensor of candidate probability ditribution; shape: (Batch Size, Latent dim)\n","    \n","    log_var: Tensor\n","        log variance of the candidate probability distribution; shape: (Batch Size, Latent dim)\n","\n","    Returns\n","    -----------------------\n","    Tensor\n","        average KL divergence loss for the batch of inputs; shape: Scalar Tensor\n","    \"\"\"\n","    loss = None\n","    ### BEGIN SOLUTION\n","    # your code here\n","    raise NotImplementedError\n","    ### END SOLUTION\n","    return loss"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"qVgSNMqhnSR-","nbgrader":{"cell_type":"code","checksum":"e733e5be2b51e9798fdc5cb45093affd","grade":true,"grade_id":"cell-4be369674ed188f1","locked":true,"points":2,"schema_version":3,"solution":false,"task":false},"tags":["Ex-3-Task-1"]},"outputs":[],"source":["### Intentionally left blank\n"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"Lr8vFaUP5Jr1","nbgrader":{"cell_type":"markdown","checksum":"e63fa716faca01dacf6ec8b789c5652f","grade":false,"grade_id":"cell-727344924c7831ca","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["## Traning the VAE"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"8i3H_Z2rW8Dd","nbgrader":{"cell_type":"code","checksum":"84a05efbddad41e9e454992036c21b05","grade":false,"grade_id":"cell-b7bc9b4c4d86bf7f","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["### READ ONLY\n","learning_rate = 5e-4\n","num_epochs = 20\n","reconstruction_loss_object = nn.MSELoss().to(device)\n","optimizer = torch.optim.Adam(vae_.parameters(), lr=learning_rate)"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"yCeSuASksNd8","nbgrader":{"cell_type":"code","checksum":"ec35e610795771bebc5cdf93955bd769","grade":false,"grade_id":"cell-c546fd3e57f4946a","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["### READ ONLY\n","\n","reconstruction_losses_avg = []\n","kl_divs_avg = []\n","losses_avg = []\n","\n","for epoch in range(num_epochs):\n","    reconstruction_losses = []\n","    kl_divs = []\n","    losses = []\n","\n","    vae_.train()\n","\n","    pbar = tqdm.notebook.tqdm(trainloader)\n","    for batch_idx, (X, _) in enumerate(pbar):\n","        X = X.to(device)\n","\n","        optimizer.zero_grad()\n","        \n","        X_reconstructed, mean, log_var = vae_(X)\n","        \n","        # compute loss(scaled by image_size for individual pixel) and KL term \n","        recon_loss = reconstruction_loss_object(X_reconstructed, X)*np.prod(image_shape)\n","        kl_loss = KLDivLoss(mean, log_var)\n","        loss = recon_loss + kl_loss\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        reconstruction_losses.append(recon_loss.item())\n","        kl_divs.append(kl_loss.item())\n","        losses.append(loss.item())\n","\n","        pbar.set_description(f\"Epoch {epoch + 1}/{num_epochs}, Batch: {batch_idx + 1}/{len(trainloader)}, Reconstruction Loss: {reconstruction_losses[-1]:}, KLDivLoss: {kl_divs[-1]}, Loss: {losses[-1]}\")\n","            \n","    reconstruction_losses_avg.append(np.mean(reconstruction_losses))\n","    kl_divs_avg.append(np.mean(kl_divs)) \n","    losses_avg.append(np.mean(losses))\n","    pbar.set_description(f\"Epoch {epoch + 1}/{num_epochs}, Batch: {batch_idx + 1}/{len(trainloader)}, Reconstruction Loss: {reconstruction_losses_avg[-1]:}, KLDivLoss: {kl_divs_avg[-1]}, Loss: {losses_avg[-1]}\")"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"Zz9rhl4uW8qr","nbgrader":{"cell_type":"markdown","checksum":"5d357e1d78506f94ff7c0a394f2163cf","grade":false,"grade_id":"cell-23608bac9e7a6717","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["## Visualization of the original image, latent variable and reconstructed image"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"CmcEP0xuXDIX","nbgrader":{"cell_type":"markdown","checksum":"ae93458f073007838e544e873017a019","grade":false,"grade_id":"cell-41d97bd24a9b1a58","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["Try visualizing the first 10 samples from test set. Your result should look like the figure shown."]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"yPvT4sxHXCi6","nbgrader":{"cell_type":"code","checksum":"28b6e5589e425b97fb33d496a166f0cb","grade":false,"grade_id":"cell-53d3107dc3496396","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["### READ ONLY\n","\n","def plotfigure(images):\n","    number_of_images = images.size()[0]\n","    plt.figure(figsize=(20, 2))\n","    for i in range(number_of_images):\n","        plot = plt.subplot(1, number_of_images, i+1)\n","        plt.imshow(images[i])\n","        plt.gray()\n","        plot.get_xaxis().set_visible(False)\n","        plot.get_yaxis().set_visible(False)\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"TiC8Xn9FXNWH","nbgrader":{"cell_type":"code","checksum":"297d9a7b616017e21b643ab512800fc1","grade":false,"grade_id":"cell-164ceb37c916ec9d","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["### READ ONLY\n","\n","num_examples = 10\n","samples = next(iter(testloader))[0][:10].to(device)\n","vae_.eval()\n","with torch.no_grad():\n","    samples_recon, _, _ = vae_(samples)\n","\n","samples = samples.permute((0, 2, 3, 1)).squeeze().cpu()\n","samples_recon = samples_recon.squeeze().cpu()\n","print('Original Test Dataset')\n","plotfigure(samples)\n","print('Reconstructed Test Dataset')\n","plotfigure(samples_recon)"]},{"cell_type":"markdown","metadata":{"id":"QnAXPPGaXN_X"},"source":["We have come to the end of the assignment. We reconstructed the Fashion MNIST images using Variational Autoencoder. Lets generate a image using the decoder of VAE we just trained."]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"Cnj9O3UgXQYN","nbgrader":{"cell_type":"code","checksum":"ebd5b4cfadc8c449086cf7902b1094db","grade":false,"grade_id":"cell-1bcce931c6e1292b","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["### READ ONLY\n","# h = torch.randn((1, latent_dim), device=device)\n","# vae_.eval()\n","# with torch.no_grad():\n","#     out = vae_.decoder(h)\n","#     out = out.permute((0, 2, 3, 1)).squeeze().cpu()\n","\n","# plt.imshow(out, cmap='gray')\n","# plt.title('Generated Digit Image')\n","# plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"pW4hMHEguvLY","nbgrader":{"cell_type":"code","checksum":"a8bb4696af0e40d7b72b21e0fa164327","grade":false,"grade_id":"cell-b56f0975e01641f6","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Assignment_VAE(PyTorch)_Student.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":0}