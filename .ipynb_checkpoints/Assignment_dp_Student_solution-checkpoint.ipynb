{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "iImtOkhQcMTj",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "91648bb21118ff19fda0738e01dc93b6",
     "grade": false,
     "grade_id": "cell-866ef40de728bcf6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Assignment: Dynamic Programming\n",
    "\n",
    "---\n",
    "\n",
    "<b><div style=\"text-align: right\">[TOTAL POINTS: 35]</div></b>\n",
    "\n",
    "In this assignment, you will implement the algorithms that you learned about in the dynamic programming chapter. Using these algorithms, you will find the optimal policy to act in a gridworld. \n",
    "\n",
    "Here are the exercises that you will solve today:\n",
    "\n",
    "* Exercise 1: Action Value Function\n",
    "\n",
    "    You will use the state values and the environment dynamics to create a function that gives the value of an action.\n",
    "\n",
    "* Exercise 2, 3, and 4: Policy Evaluation for a Single State\n",
    "\n",
    "    You will implement policy evaluation in gradual steps in three different exercises: First, you will implement policy evaluation for a single state, and then a single iteration of policy evaluation. Finally, you will implement full policy evaluation until convergence.\n",
    "\n",
    "* Exercise 5: Policy Improvement\n",
    "\n",
    "    Here, you will use greedy action selection to improve the policy. \n",
    "\n",
    "\n",
    "* Exercise 6: Policy Iteration\n",
    "\n",
    "    In this exercise, you will combine your policy evaluation and improvement implementations to run policy iteration on the environment.\n",
    "\n",
    "* Exercise 7: Value Iteration\n",
    "\n",
    "    In the final exercise, you will implement value iteration using the single sweep version of policy evaluation and policy improvement.\n",
    "\n",
    "The cell below contains two functions to aid visualization of value functions and policies. We will use them later to visualize our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "ypPTVce8cMTq",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7d1a4db464256b90a09762af97fff424",
     "grade": false,
     "grade_id": "cell-fd3599060211d8e4",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_value_function(value_function, n, goal_states, roundto=3, min_value=-1, max_value=1):\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_figheight(n)\n",
    "    fig.set_figwidth(n)\n",
    "    mat = [[value_function[j*n+i] for i in range(n)] for j in range(n)]\n",
    "    ax.matshow(mat, cmap='RdYlGn', vmin=min_value, vmax=max_value)\n",
    "    ax.axis('off')   \n",
    "\n",
    "    fig.set_figheight(5)\n",
    "    fig.set_figwidth(5)\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            ax.text(\n",
    "                i, j, str(round(value_function[j*n+i], roundto)), color='0', \n",
    "                fontsize=10, \n",
    "                horizontalalignment='center', \n",
    "                verticalalignment='center'\n",
    "            )\n",
    "def visualize_policy(policy, value_function, n, goal_states, roundto=3, min_value=-1, max_value=1):\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_figheight(n)\n",
    "    fig.set_figwidth(n)\n",
    "    mat = [[value_function[j*n+i] for i in range(n)] for j in range(n)]\n",
    "    ax.matshow(mat, cmap='RdYlGn', vmin=min_value, vmax=max_value)\n",
    "    ax.axis('off')   \n",
    "    \n",
    "    def get_policy_string(policy, state):\n",
    "        policies = [policy[state, action] for action in ACTION_SPACE]\n",
    "        return ['LEFT', 'DOWN', 'RIGHT', 'UP'][max(range(4), key=lambda x: policies[x])]\n",
    "\n",
    "    fig.set_figheight(5)\n",
    "    fig.set_figwidth(5)\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            ax.text(\n",
    "                i, j, get_policy_string(policy, i+j*n), color='0', \n",
    "                fontsize=10, \n",
    "                horizontalalignment='center', \n",
    "                verticalalignment='center'\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "oE0g26DWcMT-",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bac3212b83e414562457122df13feeb9",
     "grade": false,
     "grade_id": "cell-72308c5f9bb80fd8",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "The `Gridworld` class below defines the Markov Decision Process for this task. The two functions in this class that you will find useful are `transition_probability_function` and `reward_function`. Check how they have been implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "TeoMI294cMUE",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "165f0f95b94e2017fcf3a80f9dd89fc0",
     "grade": false,
     "grade_id": "cell-b33ea120cafd7860",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "\n",
    "    def __init__(self, n, goal_states, holes=[2,5], goal_reward=+1, hole_reward=-1, living_reward=0):\n",
    "        self.n = n\n",
    "        self.states_space = list(range(self.n*self.n))\n",
    "        self.action_space = [0,1,2,3]\n",
    "        self.goal_states = goal_states\n",
    "        self.goal_reward = goal_reward\n",
    "        self.hole_reward = hole_reward\n",
    "        self.living_reward = living_reward\n",
    "        self.holes = holes\n",
    "\n",
    "    # convenience functions to check if a state lies at an edge\n",
    "    def is_left_edge(self, state): return (state % self.n)==0\n",
    "    def is_right_edge(self, state): return ((state+1) % self.n)==0\n",
    "    def is_top_edge(self, state): return state < self.n\n",
    "    def is_bottom_edge(self, state): return self.n**2 > state > (self.n**2 - self.n)\n",
    "\n",
    "    def is_left(self, a, b):\n",
    "        # checks if state b lies to the left of a\n",
    "        if self.is_left_edge(a): return False\n",
    "        return (a-1)==b\n",
    "\n",
    "    def is_right(self, a, b):\n",
    "        # checks if state b lies to the right of a\n",
    "        if self.is_right_edge(a): return False\n",
    "        return (a+1)==b\n",
    "\n",
    "    def is_above(self, a, b):\n",
    "        # checks if state b is above a\n",
    "        if self.is_top_edge(a): return False\n",
    "        return (a-self.n)==b\n",
    "\n",
    "    def is_below(self, a, b):\n",
    "        # checks if state b is below a\n",
    "        if self.is_bottom_edge(a): return False\n",
    "        return (a+self.n)==b\n",
    "\n",
    "    def get_next_state(self, state, action):\n",
    "        # determines the state you end up at if you take\n",
    "        # action `action` at state `state`\n",
    "        if action == UP:\n",
    "            if self.is_top_edge(state): return state\n",
    "            return state - self.n\n",
    "        if action == LEFT:\n",
    "            if self.is_left_edge(state): return state\n",
    "            return state - 1\n",
    "        if action == DOWN:\n",
    "            if self.is_bottom_edge(state): return state\n",
    "            return state + self.n\n",
    "        if action == RIGHT:\n",
    "            if self.is_right_edge(state): return state\n",
    "            return state + 1\n",
    "\n",
    "\n",
    "    def reward_function(self, initial_state, action):\n",
    "\n",
    "        # our reward function is dependent on the final state\n",
    "        final_state = self.get_next_state(initial_state, action)\n",
    "\n",
    "        # goal states give a reward of -1\n",
    "        if final_state in self.goal_states: return self.goal_reward\n",
    "\n",
    "        # states with holes give a reward of -1\n",
    "        if final_state in self.holes: return self.hole_reward\n",
    "\n",
    "        # if the final state is neither a goal, nor does it have a hole\n",
    "        # we return the living reward, which is 0 here\n",
    "        # a negative living reward results in a policy that favors\n",
    "        # reaching the goal state in as little transitions as possible\n",
    "        return self.living_reward\n",
    "\n",
    "\n",
    "    def transition_probability_function(self, initial_state, action, final_state):\n",
    "\n",
    "        # if action in initial_state takes us to final_state, return 1\n",
    "        if final_state == self.get_next_state(initial_state, action):\n",
    "            return 1\n",
    "        # otherwise, return 0\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "NUfeVkAQcMUR",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c00c611e7be4e4d3a53a368dd459bf31",
     "grade": false,
     "grade_id": "cell-685817b29fc359f2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Setting up the environment\n",
    "\n",
    "Our environment is a 4x4 gridworld. It has 16 states, each identified by one of the numbers from 0 to 15. In each state, you can take on of the actions LEFT, DOWN, RIGHT and UP. These actions take you to the corresponding state in the next timestep. For example, if you take the action DOWN in the state 0, the environment takes you to state 4 in the next timestep. This transition is deterministic, which means that taking UP at state 6 *always* takes you to state 2. This simplifies our transition probability function quite a bit, which is why we are making the environment determininistic here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TtLidoHi_ub8"
   },
   "source": [
    "<div align=\"center\">\n",
    "    <figure>\n",
    "     <img src=\"https://docs.google.com/uc?export=download&id=1-u3uan1EuvUA1LimsIoSoEgWB7ch6mW3\" width=\"300\">\n",
    "     <figcaption>Our gridworld environment: In state 6, you can take one of the actions for the four directions, and the environment takes you to the corresponding state in the next timestep.</figcaption>\n",
    "    </figure>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qbQw2OruAdQL"
   },
   "source": [
    "The rewards are simple. If you somehow end up at the state 3, which we will call the goal state, you get a reward of +1. That is, the actions UP from the state 7, and RIGHT from the state 2 give you a reward of +1. Also, if you read the state 9, you get a reward of -1 because it happens to have a hole in it. \n",
    "\n",
    "All other states give you a reward of 0. The reward that an agent gets when nothing else happens is called the living reward. It is simply a reward you get for living upto the next timestep. We can select the living reward strategically to influence the policy. For example, a high living reward results in a policy that takes its time to reach the goal, since it gets a lot of reward in each state anyway. On the other hand, a low or negative living reward forces our policy to reach goals faster in order to minimize the negative rewards it gets on the way. For now, let's just put it at 0.\n",
    "\n",
    "Think of what the optimal policy for this environment should look like. An optimum policy in this environment would be one that avoids the state 9, and tries to reach state 3.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "nQp7jRk9cMUV",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7bbc78b38d3a010a058e2e74fa052a0c",
     "grade": false,
     "grade_id": "cell-94a9db216a5c8ff7",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# rewards\n",
    "GOAL_REWARD = +1\n",
    "HOLE_REWARD = -1\n",
    "LIVING_REWARD = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nVpYeZ0gCFX4"
   },
   "source": [
    "Now, let's define the action space. There are four actions you can take at any state: LEFT, DOWN, RIGHT and UP. They take you to the corresponding state in the next timestep, except when you are at an edge. If you try taking LEFT at the state 4, which is already the leftmost state in its row, you don't go anywhere, and stay at state 4 in the next timestep too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "1DReTTQjCFz5"
   },
   "outputs": [],
   "source": [
    "# actions\n",
    "LEFT, DOWN, RIGHT, UP = 0, 1, 2, 3\n",
    "\n",
    "# action space\n",
    "ACTION_SPACE = [LEFT, DOWN, RIGHT, UP]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZHh-IrOCiYn"
   },
   "source": [
    "The state space is made of 16 states like we discussed, and each state is represented by a number between 0 and 15. The image above shows the state number for each state in the grid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "OCS7oxq6CikR"
   },
   "outputs": [],
   "source": [
    "# size of the gridworld, ours is 4x4\n",
    "N = 4\n",
    "\n",
    "# state space -- each state is represented by a unique number\n",
    "STATE_SPACE = list(range(N*N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lmONCqpYC1B6"
   },
   "source": [
    "Now, let's create the environment using the GridWorld class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "w7_cVcBUC2X2"
   },
   "outputs": [],
   "source": [
    "GOAL_STATES = [3] # Top right state\n",
    "HOLES = [9] # State 3 rows from the top and 2 from right\n",
    "\n",
    "environment = GridWorld(\n",
    "    n=N,\n",
    "    goal_states=GOAL_STATES, \n",
    "    holes=HOLES, \n",
    "    goal_reward=GOAL_REWARD, \n",
    "    hole_reward=HOLE_REWARD, \n",
    "    living_reward=LIVING_REWARD\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "1KkoLql1cMUe",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "754993dc2693f2d1e9eb7ac41898da27",
     "grade": false,
     "grade_id": "cell-ff22cbbbd9a1bb4c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Parameterizing the Policy and the State Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "fttoHfsrcMUf",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3458230085ebfad0c336178e6be45c31",
     "grade": false,
     "grade_id": "cell-660eb1df588aa19a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "We will parameterize the policy as a dictionary that takes a `(state, action)` tuple as the key, and returns a number between 0 and 1 that tells us the probability that we take action `action` in state `state`. For now, let's make the policy uniform, i.e., we are equally likely to take the four actions in each state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "YelN9A5jcMUh",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "958b1cf2db3a8b48013ce7148ad7465e",
     "grade": false,
     "grade_id": "cell-a3dc457331cb3507",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "POLICY = {(state, action):0.25 for state in STATE_SPACE for action in ACTION_SPACE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "cuW2fwRFcMUq",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "64f4a2353f0c2d23d60a9f4c51ee552c",
     "grade": false,
     "grade_id": "cell-90e4db743fa7de80",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "We will also parameterize the state value function as a dictionary. Its key will be state `state`, and it gives a number that is the value of the state. Let's initialize the state value function with zeros for each state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "G7QaijIacMUt",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "57be801bfcef9c149bfe1270e099bb1c",
     "grade": false,
     "grade_id": "cell-48c17c49898e1ceb",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "STATE_VALUES = {state:0 for state in STATE_SPACE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "R6n1vKv6cMU3",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9a9b9ae81210bb134157c885074e271c",
     "grade": false,
     "grade_id": "cell-ff0835eea5716305",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Exercise 1: Action Value Function\n",
    "\n",
    "---\n",
    "\n",
    "<b><div style=\"text-align: right\">[TOTAL POINTS: 5]</div></b>\n",
    "\n",
    "In this exercise, you will use the state value function to compute the action value of an action in a state. Recall that the action value of an action in a state is the sum of the immediate reward you get when you take the action, and the expected discounted value of the next state. \n",
    "\n",
    "$$q_\\pi (s, a) = R(s,a) + \\gamma \\sum_{s' \\in S} \\text{Pr}(s'|s, a) V_\\pi(s'a)$$\n",
    "\n",
    "Implement the above equation in the function `action_value_function` given below. The value function $V_\\pi$ is in the dictionary `STATE_VALUES`. The transition probability function $\\text{Pr}(s'|s, a)$ is in the `environment.transition_probability_function`. Be sure to check the function's signature out for the correct ordering of arguments. Also, the reward function $R(s,a)$ is in the function `environment.reward_function`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "EKWNgtp_cMU4",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "08a6d173b4b915f71ac06b85c7024caf",
     "grade": false,
     "grade_id": "cell-c2e2c9e47dffa59b",
     "locked": false,
     "schema_version": 3,
     "solution": true
    },
    "tags": [
     "Ex-1-Task-1"
    ]
   },
   "outputs": [],
   "source": [
    "### Ex-1-Task-1\n",
    "GAMMA = 0.9\n",
    "\n",
    "def action_value_function(state, action):\n",
    "    ret_action_value = 0\n",
    "    ### BEGIN SOLUTION\n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "    ### END SOLUTION\n",
    "    return ret_action_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "UoihrInLcMU_",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4bca7cddc759442128b3d295aaffffb4",
     "grade": true,
     "grade_id": "cell-477c97fd722bcafc",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    },
    "tags": [
     "Ex-1-Task-1"
    ]
   },
   "outputs": [],
   "source": [
    "assert action_value_function(2, RIGHT) == 1.0\n",
    "assert action_value_function(10, LEFT) == -1.0\n",
    "assert action_value_function(4, UP) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "itqG73nUcMVF",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6b20cdad911a893a528dd8b3694bde9a",
     "grade": false,
     "grade_id": "cell-fc6382a9b6a405df",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Exercise 2: Policy evaluation for a Single State\n",
    "\n",
    "---\n",
    "\n",
    "<b><div style=\"text-align: right\">[TOTAL POINTS: 5]</div></b>\n",
    "\n",
    "In this exercise, you will implement policy evaluation for a single state. To do so, you will fill out the function `policy_evaluation_single_state`. Its input is a state, which is a number between 0 and 15, inclusive. You will need to implement the following expression. Use the action value function you built in the previous exercise to calculate $q_\\pi(s,a)$. \n",
    "\n",
    "$$V'_\\pi (s) = \\sum _{a \\in A} \\pi(a|s) q_\\pi(s, a)$$\n",
    "\n",
    "Also, **do not change any values in STATE_VALUES**. You just need to calculate the state value for the next iteration for a particular state. We will update the state value function in the next exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "lFv7aljWcMVH",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c3fcdd43ed9ca96bc189ac65f5d0e48a",
     "grade": false,
     "grade_id": "cell-4d1a175340bc2483",
     "locked": false,
     "schema_version": 3,
     "solution": true
    },
    "tags": [
     "Ex-2-Task-1"
    ]
   },
   "outputs": [],
   "source": [
    "### Ex-2-Task-1\n",
    "def policy_evaluation_single_state(state):\n",
    "    new_state_value = 0\n",
    "    ### BEGIN SOLUTION\n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "    ### END SOLUTION\n",
    "    return new_state_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "eFv4wINBcMVP",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d3607147bb5f64130d7b0f8e46d86959",
     "grade": true,
     "grade_id": "cell-fe8af80c77cb8e7d",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    },
    "tags": [
     "Ex-2-Task-1"
    ]
   },
   "outputs": [],
   "source": [
    "assert policy_evaluation_single_state(2) == 0.225\n",
    "assert policy_evaluation_single_state(10) == -0.225\n",
    "assert policy_evaluation_single_state(6) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "h95XHi47cMVX",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b26739835c717856a3985de2fbb8c128",
     "grade": false,
     "grade_id": "cell-aa7dc2ba9d3172b1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Exercise 3: Single Sweep of Policy Evaluation\n",
    "\n",
    "---\n",
    "\n",
    "<b><div style=\"text-align: right\">[TOTAL POINTS: 5]</div></b>\n",
    "\n",
    "In this exercise, you will implement a single sweep of policy evaluation for all states. To do so, you will fill out the function `policy_evaluation_single_sweep`. Roughly, here is the pseudocode you will need to follow. \n",
    "\n",
    "```\n",
    "for each state in STATE_SPACE\n",
    "    new_state_value(state) = policy_evaluation_single_state(state)    \n",
    "    \n",
    "for each state in STATE_SPACE\n",
    "    delta(state) = |state_value(state) - new_state_value(state)|\n",
    "    state_value(state) ‚Üê new_state_value(state)\n",
    "\n",
    "return max(delta)\n",
    "```\n",
    "\n",
    "Here are the implementation details for this function:\n",
    "\n",
    "1. Remember that the value of a terminal state should be 0. In this environment, there is a single terminal state, that is state 3. Make sure its value is 0. You do not need to updates its state value because its already 0 in our parameterization. \n",
    "\n",
    "2. Also, you will need to find out the largest absolute change in the value function for all states, and return it. In the code cell below, we have named the variable `delta`. \n",
    "\n",
    "3. Make sure to not change the value function of any state during the evaluation stage. Calculate the value function for each state and store it somewhere, and when the calculations are done, update all the values at once. The pseudocode above shows you roughly how you can do it. If you update in-place, **you will fail this exercise and others that depend on this function**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "6OvEPcPNcMVZ",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b357c2868e1ad7374dcb36037b11bcb2",
     "grade": false,
     "grade_id": "cell-285bf7ffcae735e7",
     "locked": false,
     "schema_version": 3,
     "solution": true
    },
    "tags": [
     "Ex-3-Task-1"
    ]
   },
   "outputs": [],
   "source": [
    "### Ex-3-Task-1\n",
    "def policy_evaluation_single_sweep():\n",
    "    delta = 0\n",
    "    ### BEGIN SOLUTION\n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "    ### END SOLUTION\n",
    "    return delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "-v2WEXX3cMVg",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4e884002315b9e3fa8249b214cb9726b",
     "grade": true,
     "grade_id": "cell-5dc03298294c589e",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    },
    "tags": [
     "Ex-3-Task-1"
    ]
   },
   "outputs": [],
   "source": [
    "delta = policy_evaluation_single_sweep()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C2P3vGpO4cUt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "y6J9BPxQcMVq",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bffc5599a29c3fec3bd068dbc4813427",
     "grade": false,
     "grade_id": "cell-e24e1b6b7bdbc873",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Here is the state value function after one iteration. Notice how the cells around the hole have a negative value, and the cells around the goal state have a value higher than the default 0. In a single iteration, the algorithm can only update values using the value of the states you can reach in the next timestep, so only states with some immediate reward have changed. \n",
    "\n",
    "If you run this cell more than once, you can see the how the state value function changes with more and more iterations of polict evaluation. Be sure to check it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "evwnb1EXcMVs",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f891a63bf93deb33c926ddbd15e7a074",
     "grade": false,
     "grade_id": "cell-418c89ffd16605d0",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "visualize_value_function(STATE_VALUES, 4, [3])\n",
    "delta = policy_evaluation_single_sweep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "hWql56dtcMV2",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "154aefc35f5ee1a42b4b9ba61d0a5f0d",
     "grade": false,
     "grade_id": "cell-3e87cd879999b974",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Exercise 4: Policy Evaluation until Convergence\n",
    "\n",
    "---\n",
    "\n",
    "<b><div style=\"text-align: right\">[TOTAL POINTS: 5]</div></b>\n",
    "\n",
    "In this exercise, you will implement policy evaluation until convergence. To do so, you will fill out the function `policy_evaluation`. You will use the function `policy_evaluation_single_sweep` that you built above to do this.  \n",
    "\n",
    "When the change in the value function is smaller than `MAX_DELTA`, you can consider that the value function has converged, and stop the iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "CRx9uOpMcMV3",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9b653ef4123623988554e123692eede7",
     "grade": false,
     "grade_id": "cell-2a87474620e6867f",
     "locked": false,
     "schema_version": 3,
     "solution": true
    },
    "tags": [
     "Ex-4-Task-1"
    ]
   },
   "outputs": [],
   "source": [
    "### Ex-4-Task-1\n",
    "\n",
    "STATE_VALUES = {state:0 for state in STATE_SPACE}\n",
    "MAX_DELTA = 0.001\n",
    "\n",
    "def policy_evaluation():\n",
    "    ### BEGIN SOLUTION\n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "p7CZsRRwcMWB",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a17f580cf0f8e954c5044ef00b88c90a",
     "grade": true,
     "grade_id": "cell-10d1082fa3a36de4",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    },
    "tags": [
     "Ex-4-Task-1"
    ]
   },
   "outputs": [],
   "source": [
    "policy_evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "66clQqoQcMWI",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e117665128be9e253fa4d4609ef2b2c1",
     "grade": false,
     "grade_id": "cell-5edc4880b6b3286a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Here is the value function after convergence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "QqfkTXoQcMWJ",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3ae54c4ef0a92979d97726914469bcbd",
     "grade": false,
     "grade_id": "cell-90b1f1a2c39f0546",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "visualize_value_function(STATE_VALUES, 4, [3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "kIyHf2LCcMWT",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c92d5c1b84fbc8a582a04fff62bc827d",
     "grade": false,
     "grade_id": "cell-6d632355179b6ce5",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Exercise 5: Policy Improvement\n",
    "\n",
    "---\n",
    "\n",
    "<b><div style=\"text-align: right\">[TOTAL POINTS: 5]</div></b>\n",
    "\n",
    "In this exercise, you will implement policy improvement. To do so, you will fill out the function `policy_improvement`. The expression for greedily improving the policy is given below. Use the expression to greedily improve the policy for each state in the state space. Remember that the policy is implemented as the `POLICY` dictionary. Therefore, you will need to mutate it. \n",
    "\n",
    "$$\n",
    "    \\pi(s,a)= \n",
    "\\begin{cases}\n",
    "    1, & \\text{if } q_\\pi(s,a) = max_a\\ q_\\pi(s,a)\\\\\n",
    "    0,              & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "In this gridworld, a single state can have more than 1 optimum action. Make sure that all the optimal actions in the same state do not have a probability of 1 in the `POLICY` dictionary. To do this, you can do one of the following:\n",
    "\n",
    "1. Set one of the optimal actions to have a probability of 1, and set all the other actions to have probabilities of 0. \n",
    "\n",
    "2. Set all the optimal actions to have the same probability. For example, in state 6, both UP and RIGHT can be equally optimal if the states 2 and 7 both point to the goal state 3. You can then set `POLICY` in this way:\n",
    "\n",
    "```\n",
    "POLICY(state=6, action=UP)    = 0.5\n",
    "POLICY(state=6, action=RIGHT) = 0.5\n",
    "POLICY(state=7, action=LEFT)  = 0\n",
    "POLICY(state=7, action=DOWN)  = 0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "RDYyQNNmcMWU",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2ea59b5ea357e77a676e7afc644b7d80",
     "grade": false,
     "grade_id": "cell-e049bdaace3a4378",
     "locked": false,
     "schema_version": 3,
     "solution": true
    },
    "tags": [
     "Ex-5-Task-1"
    ]
   },
   "outputs": [],
   "source": [
    "### Ex-5-Task-1\n",
    "def policy_improvement():\n",
    "    ### BEGIN SOLUTION\n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "QIqsjvaVcMWa",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1d7b52557f9bb0f79dfe08d493d8d755",
     "grade": true,
     "grade_id": "cell-78cdefcbfbfc2c54",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    },
    "tags": [
     "Ex-5-Task-1"
    ]
   },
   "outputs": [],
   "source": [
    "policy_improvement()\n",
    "visualize_policy(POLICY, STATE_VALUES, 4, [3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "CZHgevGncMWg",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a2c76d02c3d358af62ef463816511fba",
     "grade": false,
     "grade_id": "cell-38fcd16396f3afea",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Exercise 6: Policy Iteration\n",
    "\n",
    "---\n",
    "\n",
    "<b><div style=\"text-align: right\">[TOTAL POINTS: 5]</div></b>\n",
    "\n",
    "In this exercise, you will implement policy iteration. To do so, you will fill out the function `policy_iteration`. \n",
    "\n",
    "In the cell immediately below this, you will find the function `is_policy_stable`. It takes in two policies, and returns `True` if the two policies are the same. \n",
    "\n",
    "In the `policy_iteration` function, the `is_policy_stable` function has been used to find out if policy iteration has converged. You will need to fill out the actual steps of the Policy Iteration algorithm. Make sure to use the functions you have built in the exercises above, the solution to this exercise is just a couple of function calls, that is it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "Ux11moTacMWh",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "296f096afb6f5c59cf5cc0a270b2ec8a",
     "grade": false,
     "grade_id": "cell-380eb0081bd974c6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def is_policy_stable(old_policy, new_policy):\n",
    "    for state in STATE_SPACE:\n",
    "        for action in ACTION_SPACE:\n",
    "            if old_policy[state, action] != new_policy[state, action]:\n",
    "                return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "G0e6gXCkcMWp",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e40f9fb77339d1558d29c1d25f59fcee",
     "grade": false,
     "grade_id": "cell-1cff54c949244167",
     "locked": false,
     "schema_version": 3,
     "solution": true
    },
    "tags": [
     "Ex-6-Task-1"
    ]
   },
   "outputs": [],
   "source": [
    "### Ex-6-Task-1\n",
    "def policy_iteration():\n",
    "    policy_stable = False\n",
    "    while not policy_stable:\n",
    "        old_policy = dict(POLICY)\n",
    "        ### BEGIN SOLUTION\n",
    "        # your code here\n",
    "        raise NotImplementedError\n",
    "        ### END SOLUTION\n",
    "        new_policy = dict(POLICY)\n",
    "        policy_stable = is_policy_stable(old_policy, new_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "YUq9gwHMcMWv",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "738fce48525b7c34f7021c9c6cc022b4",
     "grade": true,
     "grade_id": "cell-47f1e86a25ac3942",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    },
    "tags": [
     "Ex-6-Task-1"
    ]
   },
   "outputs": [],
   "source": [
    "POLICY = {(state, action):0.25 for state in STATE_SPACE for action in ACTION_SPACE}\n",
    "STATE_VALUES = {state:0 for state in STATE_SPACE}\n",
    "policy_iteration()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "YLUiOS3rcMW1",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c0700e1c5db8bb996588df6f06c86b84",
     "grade": false,
     "grade_id": "cell-29bbf1c41fbbafb6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Here is the optimal policy calculated by your policy iteration implementation. If your implementation is correct, the policy should avoid the hole, and the value of all the states should be positive (green here). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "wEyps-nvcMW2",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "410e485a39e0e1fdad60fbbd1b255960",
     "grade": false,
     "grade_id": "cell-cc88332f8e84e63f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "visualize_policy(POLICY, STATE_VALUES, 4, [3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "KcPFl96AcMW_",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6aeb7e9aa050531cd1c0f70e822ff3b1",
     "grade": false,
     "grade_id": "cell-6dd172f29f30a3a6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Exercise 7: Value Iteration\n",
    "\n",
    "---\n",
    "\n",
    "<b><div style=\"text-align: right\">[TOTAL POINTS: 5]</div></b>\n",
    "\n",
    "In this exercise, you will implement value iteration. To do so, you will fill out the function `value_iteration`. \n",
    "\n",
    "In value iteration, recall that you need to do a single sweep of policy evaluation, and then improve the policy immediately. Convergence is, again, determined by the `MAX_DELTA` parameter, so run the iterations until the difference between consecutive value functions is smaller than `MAX_DELTA`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "0rdF1zMocMXB",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bdae9807f4372e780e79faf01cec63ac",
     "grade": false,
     "grade_id": "cell-8e7f5c3114eeaefd",
     "locked": false,
     "schema_version": 3,
     "solution": true
    },
    "tags": [
     "Ex-7-Task-1"
    ]
   },
   "outputs": [],
   "source": [
    "### Ex-7-Task-1\n",
    "\n",
    "def value_iteration():\n",
    "    MAX_DELTA = 0.01\n",
    "    ### BEGIN SOLUTION\n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "yKN4Wph1cMXG",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7e91a88406189a17a70eacf68d465621",
     "grade": true,
     "grade_id": "cell-05dd932954dee3fe",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    },
    "tags": [
     "Ex-7-Task-1"
    ]
   },
   "outputs": [],
   "source": [
    "POLICY = {(state, action):0.25 for state in STATE_SPACE for action in ACTION_SPACE}\n",
    "STATE_VALUES = {state:0 for state in STATE_SPACE}\n",
    "\n",
    "value_iteration()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "ACxT0bDFcMXP",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "88731bdcebc85c0a59b7f0e1531085ea",
     "grade": false,
     "grade_id": "cell-8d1e5a175af82312",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Just to be sure, here is the optimal policy and its value function calculated by the value iteration algorithm you implemented. Make sure the results match that of policy iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "t4kRihfPcMXR",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "17062621c019e223b726aa2329f0ec16",
     "grade": false,
     "grade_id": "cell-415f5b3426a706a4",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "visualize_policy(POLICY, STATE_VALUES, 4, [3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "LZVdCUuvcMXW",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "36a1a911584cb5241ec734c72bf19bea",
     "grade": false,
     "grade_id": "cell-aeac1b7284d56c41",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this assignment, you have implemented different algorithms that use dynamic programming to find the optimal policy and value function for an environment. We hope that you have truly understood the algorithms in this chapter now. Getting an intuitive feel for what they are doing will help you understand the concepts in the next few chapters. Congratulations on completing the assignment!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment-dp_Student.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
